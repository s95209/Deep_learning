{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 20:43:59.425350: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 20:44:01.367373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.367494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.371760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.371863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.371955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.372043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.372723: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 20:44:01.373735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.373843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.373934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.746436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.746574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.746668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-12-19 20:44:01.746741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10073 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5427 vocabularies in total\n",
      "Word to id mapping, for example: flower -> 1\n",
      "Id to word mapping, for example: 1 -> flower\n",
      "Tokens: <PAD>: 5427; <RARE>: 5428\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = './dictionary'\n",
    "vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption2text(x):\n",
    "    sentence_list = []\n",
    "    for _, id_sentence in enumerate(x):\n",
    "        word_str = \"\"\n",
    "        for word_id in id_sentence:\n",
    "            if (id2word_dict[word_id] == '<PAD>'):\n",
    "                break\n",
    "            if (id2word_dict[word_id] == '<RARE>'):\n",
    "                continue\n",
    "            word_str = word_str + id2word_dict[word_id] + ' ' \n",
    "        # print(word_str)\n",
    "        sentence_list.append(word_str)\n",
    "    return sentence_list\n",
    "\n",
    "def text2Embeddings(sentence_list):\n",
    "    embeddings_list = []\n",
    "    for i, sentence in enumerate(sentence_list):\n",
    "        embeddings = sbert.encode(sentence)\n",
    "        embeddings_list.append(embeddings)\n",
    "    return embeddings_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "pretrain_embedding_weight_path = 'train_data_embedding_stsb.pkl'\n",
    "pretrained_weight = 'sentence-transformers/stsb-bert-large'\n",
    "# pretrain_embedding_weight = 'train_data_embedding_mpnet.pkl'\n",
    "data_path = './dataset/'\n",
    "if(os.path.exists(data_path + pretrain_embedding_weight_path)):\n",
    "    df_train = pd.read_pickle(data_path + pretrain_embedding_weight_path)\n",
    "else:\n",
    "    sbert = SentenceTransformer(pretrained_weight)\n",
    "    df_train = pd.read_pickle(data_path + 'text2ImgData.pkl')\n",
    "    df_train['Texts'] = df_train['Captions'].apply(caption2text)\n",
    "    df_train['Embeddings'] = df_train['Texts'].apply(text2Embeddings)\n",
    "    df_train.to_pickle(data_path + pretrain_embedding_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Captions</th>\n",
       "      <th>ImagePath</th>\n",
       "      <th>Texts</th>\n",
       "      <th>Embeddings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>[[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...</td>\n",
       "      <td>./102flowers/image_06734.jpg</td>\n",
       "      <td>[the petals of the flower are pink in color an...</td>\n",
       "      <td>[[-0.20267655, -0.2748266, 0.5756946, -1.04107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>[[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...</td>\n",
       "      <td>./102flowers/image_06736.jpg</td>\n",
       "      <td>[this flower has white petals and yellow pisti...</td>\n",
       "      <td>[[0.02512608, 0.14785601, 0.47533122, -0.96326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6737</th>\n",
       "      <td>[[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...</td>\n",
       "      <td>./102flowers/image_06737.jpg</td>\n",
       "      <td>[the petals on this flower are pink with white...</td>\n",
       "      <td>[[0.05587807, -0.37910694, 0.1740157, -1.08987...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>[[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...</td>\n",
       "      <td>./102flowers/image_06738.jpg</td>\n",
       "      <td>[the flower has a smooth purple petal with whi...</td>\n",
       "      <td>[[-0.18846968, -0.72733647, 0.13720995, 0.0410...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6739</th>\n",
       "      <td>[[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...</td>\n",
       "      <td>./102flowers/image_06739.jpg</td>\n",
       "      <td>[this white flower has bright yellow stamen wi...</td>\n",
       "      <td>[[0.24943687, -0.10841094, 0.089149155, -0.656...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Captions  \\\n",
       "ID                                                        \n",
       "6734  [[9, 2, 17, 9, 1, 6, 14, 13, 18, 3, 41, 8, 11,...   \n",
       "6736  [[4, 1, 5, 12, 2, 3, 11, 31, 28, 68, 106, 132,...   \n",
       "6737  [[9, 2, 27, 4, 1, 6, 14, 7, 12, 19, 5427, 5427...   \n",
       "6738  [[9, 1, 5, 8, 54, 16, 38, 7, 12, 116, 325, 3, ...   \n",
       "6739  [[4, 12, 1, 5, 29, 11, 19, 7, 26, 70, 5427, 54...   \n",
       "\n",
       "                         ImagePath  \\\n",
       "ID                                   \n",
       "6734  ./102flowers/image_06734.jpg   \n",
       "6736  ./102flowers/image_06736.jpg   \n",
       "6737  ./102flowers/image_06737.jpg   \n",
       "6738  ./102flowers/image_06738.jpg   \n",
       "6739  ./102flowers/image_06739.jpg   \n",
       "\n",
       "                                                  Texts  \\\n",
       "ID                                                        \n",
       "6734  [the petals of the flower are pink in color an...   \n",
       "6736  [this flower has white petals and yellow pisti...   \n",
       "6737  [the petals on this flower are pink with white...   \n",
       "6738  [the flower has a smooth purple petal with whi...   \n",
       "6739  [this white flower has bright yellow stamen wi...   \n",
       "\n",
       "                                             Embeddings  \n",
       "ID                                                       \n",
       "6734  [[-0.20267655, -0.2748266, 0.5756946, -1.04107...  \n",
       "6736  [[0.02512608, 0.14785601, 0.47533122, -0.96326...  \n",
       "6737  [[0.05587807, -0.37910694, 0.1740157, -1.08987...  \n",
       "6738  [[-0.18846968, -0.72733647, 0.13720995, 0.0410...  \n",
       "6739  [[0.24943687, -0.10841094, 0.089149155, -0.656...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 1024\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "# Stage 1 dataset generator\n",
    "def map_train_func(image_path, embedding):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=IMAGE_CHANNEL)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, IMAGE_CHANNEL])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "\n",
    "    if tf.random.uniform([]) < 0.25:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "    if tf.random.uniform([]) < 0.25:\n",
    "        img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = img * 2 - 1\n",
    "\n",
    "    return img, embedding\n",
    "\n",
    "# def training_dataset_generator(image_path_df, embedding_df, batch_size):\n",
    "#     image_path_list = []\n",
    "#     embedding_list = []\n",
    "#     for image_path, embeddings in zip(image_path_df.values, embedding_df.values):\n",
    "#         for embedding in embeddings: # since there are multiple embeddings (sentences) into one image.\n",
    "#             image_path_list.append(image_path)\n",
    "#             embedding_list.append(embedding)\n",
    "\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((image_path_list, embedding_list)) # return a image and a embedding.\n",
    "#     dataset = dataset.map(map_train_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(len(embedding_list)).batch(batch_size, drop_remainder=True)\n",
    "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#     return dataset\n",
    "\n",
    "def training_dataset_generator(image_path_df, embedding_df, batch_size):\n",
    "    image_path_list = []\n",
    "    embedding_list = []\n",
    "    for image_path, embeddings in zip(image_path_df.values, embedding_df.values):\n",
    "        # for embedding in embeddings: # since there are multiple embeddings (sentences) into one image.\n",
    "        image_path_list.append(image_path)\n",
    "        embedding_list.append(random.choice(embeddings))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_path_list, embedding_list)) # return a image and a embedding.\n",
    "    dataset = dataset.map(map_train_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(embedding_list)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(embedding, index):\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "    return embedding, index\n",
    "\n",
    "\n",
    "def test_dataset_generator(embedding_array, index_array, batch_size):\n",
    "    embedding_list = []\n",
    "    index_list = []\n",
    "    for embedding, index in zip(embedding_array, index_array):\n",
    "        embedding_list.append(embedding)\n",
    "        index_list.append(index)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((embedding_list, index_list))\n",
    "    dataset = dataset.map(map_test, tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def testing_data_generator(caption, index):\n",
    "#     caption = tf.cast(caption, tf.float32)\n",
    "#     caption_embedding = caption_embedding_model(caption)\n",
    "#     caption_embedding = tf.reduce_mean(caption_embedding, axis=0)\n",
    "#     return caption, caption_embedding\n",
    "\n",
    "# def testing_dataset_generator(batch_size, data_generator):\n",
    "#     data = pd.read_pickle('./dataset/testData.pkl')\n",
    "#     captions = data['Captions'].values\n",
    "#     caption = []\n",
    "#     for i in range(len(captions)):\n",
    "#         caption.append(captions[i])\n",
    "#     caption = np.asarray(caption)\n",
    "#     caption = caption.astype(np.int)\n",
    "#     index = data['ID'].values\n",
    "#     index = np.asarray(index)\n",
    "    \n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((caption, index))\n",
    "#     dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_IMAGE_HEIGHT = 256\n",
    "# H_IMAGE_WIDTH = 256\n",
    "# H_IMAGE_CHANNEL = 3\n",
    "\n",
    "# caption_embedding_model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(input_dim=len(word2Id_dict), output_dim=EMBEDDING_DIM, input_length=20),\n",
    "# ])\n",
    "\n",
    "# def H_training_data_generator(image_path, caption):\n",
    "#     # load in the image according to image path\n",
    "#     img = tf.io.read_file(image_path)\n",
    "#     img = tf.image.decode_image(img, channels=3)\n",
    "#     img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "#     img.set_shape([None, None, 3])\n",
    "#     img = tf.image.resize(img, size=[H_IMAGE_HEIGHT, H_IMAGE_WIDTH])\n",
    "#     img.set_shape([H_IMAGE_HEIGHT, H_IMAGE_WIDTH, H_IMAGE_CHANNEL])\n",
    "#     caption = tf.cast(caption, tf.int32)\n",
    "#     caption_embedding = caption_embedding_model(caption)\n",
    "#     caption_embedding = tf.reduce_mean(caption_embedding, axis=0)\n",
    "    \n",
    "#     return img, caption_embedding\n",
    "\n",
    "# def H_dataset_generator(filenames, batch_size, data_generator):\n",
    "#     # load the training data into two NumPy arrays\n",
    "#     df = pd.read_pickle(filenames)\n",
    "#     captions = df['Captions'].values\n",
    "#     caption = []\n",
    "#     # each image has 1 to 10 corresponding captions\n",
    "#     # we choose one of them randomly for training\n",
    "#     for i in range(len(captions)):\n",
    "#         caption.append(random.choice(captions[i]))\n",
    "#     caption = np.asarray(caption)\n",
    "#     caption = caption.astype(np.int)\n",
    "#     image_path = df['ImagePath'].values\n",
    "    \n",
    "#     # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "#     assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((image_path, caption))\n",
    "#     dataset = dataset.map(data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(len(caption)).batch(batch_size, drop_remainder=True)\n",
    "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "#     return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_IMAGE_HEIGHT = 256\n",
    "H_IMAGE_WIDTH = 256\n",
    "H_IMAGE_CHANNEL = 3\n",
    "\n",
    "# Stage 2 dataset generator\n",
    "def stage2_map_train_func(image_path, embedding):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=H_IMAGE_CHANNEL)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, H_IMAGE_CHANNEL])\n",
    "    img = tf.image.resize(img, size=[H_IMAGE_HEIGHT, H_IMAGE_WIDTH])\n",
    "    img.set_shape([H_IMAGE_HEIGHT, H_IMAGE_WIDTH, H_IMAGE_CHANNEL])\n",
    "\n",
    "    embedding = tf.cast(embedding, tf.float32)\n",
    "\n",
    "\n",
    "    if tf.random.uniform([]) < 0.25:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "    if tf.random.uniform([]) < 0.25:\n",
    "        img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = img * 2 - 1\n",
    "\n",
    "    return img, embedding\n",
    "\n",
    "# def stage2_training_dataset_generator(image_path_df, embedding_df, batch_size):\n",
    "#     image_path_list = []\n",
    "#     embedding_list = []\n",
    "#     for image_path, embeddings in zip(image_path_df.values, embedding_df.values):\n",
    "#         for embedding in embeddings: # since there are multiple embeddings (sentences) into one image.\n",
    "#             image_path_list.append(image_path)\n",
    "#             embedding_list.append(embedding)\n",
    "\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((image_path_list, embedding_list)) # return a image and a embedding.\n",
    "#     dataset = dataset.map(stage2_map_train_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     dataset = dataset.shuffle(len(embedding_list)).batch(batch_size, drop_remainder=True)\n",
    "#     dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#     return dataset\n",
    "\n",
    "def stage2_training_dataset_generator(image_path_df, embedding_df, batch_size):\n",
    "    image_path_list = []\n",
    "    embedding_list = []\n",
    "    for image_path, embeddings in zip(image_path_df.values, embedding_df.values):\n",
    "        image_path_list.append(image_path)\n",
    "        embedding_list.append(random.choice(embeddings))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_path_list, embedding_list)) # return a image and a embedding.\n",
    "    dataset = dataset.map(stage2_map_train_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.shuffle(len(embedding_list)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_c(x):\n",
    "    mean = x[:,:128]\n",
    "    log_sigma = x[:,128:]\n",
    "    stddev = tf.exp(log_sigma)\n",
    "    epsilon = tf.random.normal((mean.shape[1],),dtype=tf.int32)\n",
    "    c = stddev * epsilon + mean\n",
    "    return c\n",
    "\n",
    "class CA(keras.Model):\n",
    "    \"\"\"\n",
    "    Get conditioning augmentation model.\n",
    "    Takes an embedding of shape (1024,) and returns a tensor of shape (256,)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CA,self).__init__()\n",
    "        self.fc = layers.Dense(256)\n",
    "        self.activation  = layers.LeakyReLU(alpha=0.2)\n",
    "    def call(self,inputs,training=False):\n",
    "        x = self.activation(self.fc(inputs))\n",
    "        return x\n",
    "\n",
    "class Embedding_Compressor(keras.Model):\n",
    "    \"\"\"\n",
    "    Build embedding compressor model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Embedding_Compressor,self).__init__()\n",
    "        self.fc = layers.Dense(128)\n",
    "        self.activation = layers.ReLU()\n",
    "    def call(self,inputs,training=False):\n",
    "        x = self.activation(self.fc(inputs))\n",
    "        return x\n",
    "\n",
    "class Generator_stage1(keras.Model):\n",
    "    \"\"\"\n",
    "    Builds a generator model used in Stage-I\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Generator_stage1,self).__init__()\n",
    "        self.ca_fc = layers.Dense(256)\n",
    "        self.ca_activation = layers.LeakyReLU(alpha=0.2)\n",
    "        #self.lambda1 = layers.Lambda(generator_c)\n",
    "        #self.mean1 = layers.Dense(128)\n",
    "        #self.log_sigma1 = layers.Dense(128)\n",
    "        self.fc1 = layers.Dense(128 * 8 * 4 * 4,use_bias=False)\n",
    "        self.activation = layers.ReLU()\n",
    "        \n",
    "        self.upsampling1 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv1 = layers.Conv2D(512,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.ac1 = layers.ReLU()\n",
    "        \n",
    "        self.upsampling2 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv2 = layers.Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.ac2 = layers.ReLU()\n",
    "\n",
    "        self.upsampling3 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv3 = layers.Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.ac3 = layers.ReLU()\n",
    "\n",
    "        self.upsampling4 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv4 = layers.Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.ac4 = layers.ReLU()\n",
    "\n",
    "        self.conv5 = layers.Conv2D(3,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    def call(self,inputs,training=False):\n",
    "        mean_logsigma = tf.split(self.ca_activation(self.ca_fc(inputs[0])),num_or_size_splits=2,axis=-1)\n",
    "        #print(mean_logsigma.shape)\n",
    "        #c = self.lambda1(mean_logsigma)\n",
    "        #mean_logsigma_split = tf.split(mean_logsigma,num_or_size_splits=2,axis=-1)\n",
    "        mean = mean_logsigma[0]\n",
    "        log_sigma = mean_logsigma[1]\n",
    "        stddev = tf.exp(log_sigma)\n",
    "        c = stddev * inputs[2] + mean\n",
    "        #print(c.shape)\n",
    "        gen_inputs = tf.concat([c,inputs[1]],axis=1)\n",
    "        #print(gen_inputs.shape)\n",
    "        x = self.activation(self.fc1(gen_inputs))\n",
    "        #print(x.shape)\n",
    "        x = tf.reshape(x,shape=(-1,4,4,128*8))\n",
    "        x = self.ac1(self.bn1(self.conv1(self.upsampling1(x)),training=training))\n",
    "        x = self.ac2(self.bn2(self.conv2(self.upsampling2(x)),training=training))\n",
    "        x = self.ac3(self.bn3(self.conv3(self.upsampling3(x)),training=training))\n",
    "        x = self.ac4(self.bn4(self.conv4(self.upsampling4(x)),training=training))\n",
    "        x = self.conv5(x)\n",
    "        x = tf.tanh(x)\n",
    "        #print(x.shape)\n",
    "        return x,mean_logsigma\n",
    "\n",
    "class Discriminator_stage1(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator_stage1,self).__init__()\n",
    "        self.e_fc = layers.Dense(128)\n",
    "        self.e_ac = layers.LeakyReLU(alpha=0.2)\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(64,kernel_size=(4,4),padding='same',strides=2,use_bias=False)\n",
    "        self.ac1 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv2 = layers.Conv2D(128,kernel_size=(4,4),padding='same',strides=2,use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.ac2 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv3 = layers.Conv2D(256,kernel_size=(4,4),padding='same',strides=2,use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.ac3 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv4 = layers.Conv2D(512,kernel_size=(4,4),padding='same',strides=2,use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.ac4 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv5 = layers.Conv2D(512,kernel_size=1,padding='same',strides=1)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.ac5 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "    def call(self,inputs,training=False):\n",
    "        x = self.ac1(self.conv1(inputs[0]))\n",
    "        #print(x.shape)\n",
    "        x = self.ac2(self.bn1(self.conv2(x),training=training))\n",
    "        #print(x.shape)\n",
    "        x = self.ac3(self.bn2(self.conv3(x),training=training))\n",
    "        #print(x.shape)\n",
    "        x = self.ac4(self.bn3(self.conv4(x),training=training))\n",
    "        #print(x.shape)\n",
    "        #print(x.shape)\n",
    "        input_layer2 = self.e_ac(self.e_fc(inputs[1]))\n",
    "        #print(input_layer2.shape)\n",
    "        input_layer2 = tf.reshape(input_layer2,shape=(-1,1,1,128))\n",
    "        #print(input_layer2.shape)\n",
    "        input_layer2 = tf.tile(input_layer2,[1,4,4,1])\n",
    "        #print(input_layer2.shape)\n",
    "        x = tf.concat([x,input_layer2],axis=-1)\n",
    "        #print(x.shape)\n",
    "        x = self.ac5(self.bn4(self.conv5(x),training=training))\n",
    "        #print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        #print(x.shape)\n",
    "        x = tf.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_block(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Residual_block,self).__init__()\n",
    "        self.conv1 = layers.Conv2D(128*4,kernel_size=(3,3),padding='same',strides=1)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.ac1 = layers.ReLU()\n",
    "        self.conv2 = layers.Conv2D(128*4,kernel_size=(3,3),padding='same',strides=1)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.ac2 = layers.ReLU()\n",
    "\n",
    "    def call(self,inputs,training=False):\n",
    "        x = self.bn1(self.conv1(inputs),training=training)\n",
    "        x = self.ac1(x)\n",
    "        x = self.bn2(self.conv2(x),training=training)\n",
    "        x = layers.add([x,inputs])\n",
    "        x = self.ac2(x)\n",
    "        return x\n",
    "\n",
    "class Generator_stage2(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator_stage2,self).__init__()\n",
    "        self.ca_fc = layers.Dense(256)\n",
    "        self.ca_activation = layers.LeakyReLU(alpha=0.2)\n",
    "        #self.mean1 = layers.Dense(128)\n",
    "        #self.log_sigma1 = layers.Dense(128)\n",
    "\n",
    "        self.conv1 = layers.Conv2D(128,kernel_size=(3,3),strides=1,padding='same',use_bias=False)\n",
    "        self.ac1 = layers.ReLU()\n",
    "        self.conv2 = layers.Conv2D(256,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.ac2 = layers.ReLU()\n",
    "        self.conv3 = layers.Conv2D(512,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.ac3 = layers.ReLU()\n",
    "\n",
    "        self.conv4 = layers.Conv2D(512,kernel_size=(3,3),strides=1,padding='same',use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.ac4 = layers.ReLU()\n",
    "\n",
    "        self.rb1 = Residual_block()\n",
    "        self.rb2 = Residual_block()\n",
    "        self.rb3 = Residual_block()\n",
    "        self.rb4 = Residual_block()\n",
    "        \n",
    "        self.upsampling1 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv5 = layers.Conv2D(512,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.ac5 = layers.ReLU()\n",
    "\n",
    "        self.upsampling2 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv6 = layers.Conv2D(256,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.ac6 = layers.ReLU()\n",
    "\n",
    "        self.upsampling3 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv7 = layers.Conv2D(128,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn6 = layers.BatchNormalization()\n",
    "        self.ac7 = layers.ReLU()\n",
    "\n",
    "        self.upsampling4 = layers.UpSampling2D(size=(2,2))\n",
    "        self.conv8 = layers.Conv2D(64,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "        self.bn7 = layers.BatchNormalization()\n",
    "        self.ac8 = layers.ReLU()\n",
    "\n",
    "        self.conv9 = layers.Conv2D(3,kernel_size=3,strides=1,padding='same',use_bias=False)\n",
    "\n",
    "    def call(self,inputs,training):\n",
    "        #CA Network\n",
    "        mean_logsigma = tf.split(self.ca_activation(self.ca_fc(inputs[0])),num_or_size_splits=2,axis=-1)\n",
    "        #mean_logsigma = self.ca_activation(self.ca_fc(inputs[0]))\n",
    "        mean = mean_logsigma[0]\n",
    "        log_sigma = mean_logsigma[1]\n",
    "        stddev = tf.exp(log_sigma)\n",
    "        c = stddev * inputs[2] + mean\n",
    "        #c = tf.concat([c,inputs[1]],axis=1)\n",
    "        #Image Encoder\n",
    "        x = self.ac1(self.conv1(inputs[1]))\n",
    "        x = self.ac2(self.bn1(self.conv2(x),training=training))\n",
    "        x = self.ac3(self.bn2(self.conv3(x),training=training))\n",
    "        c = tf.expand_dims(c,axis=1)\n",
    "        c = tf.expand_dims(c,axis=1)\n",
    "        c = tf.tile(c,[1,16,16,1])\n",
    "        #Concatenation\n",
    "        c_code = tf.concat([c,x],axis=3)\n",
    "        #Residual Block\n",
    "        x = self.ac4(self.bn3(self.conv4(c_code),training=training))\n",
    "        x = self.rb1(x)\n",
    "        x = self.rb2(x)\n",
    "        x = self.rb3(x)\n",
    "        x = self.rb4(x)\n",
    "        #Upsampling block\n",
    "        x = self.ac5(self.bn4(self.conv5(self.upsampling1(x)),training=training))\n",
    "        x = self.ac6(self.bn5(self.conv6(self.upsampling2(x)),training=training))\n",
    "        x = self.ac7(self.bn6(self.conv7(self.upsampling3(x)),training=training))\n",
    "        x = self.ac8(self.bn7(self.conv8(self.upsampling4(x)),training=training))\n",
    "        x = self.conv9(x)\n",
    "        x = tf.tanh(x)\n",
    "        return x,mean_logsigma\n",
    "\n",
    "class Discriminator_stage2(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator_stage2,self).__init__()\n",
    "        self.e_fc = layers.Dense(128)\n",
    "        self.e_ac = layers.LeakyReLU(alpha=0.2)\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(64,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.ac1 = layers.LeakyReLU(alpha=0.2)\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(128,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.ac2 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv3 = layers.Conv2D(256,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.ac3 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv4 = layers.Conv2D(512,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.ac4 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv5 = layers.Conv2D(1024,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.ac5 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv6 = layers.Conv2D(2048,kernel_size=(4,4),strides=2,padding='same',use_bias=False)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.ac6 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv7 = layers.Conv2D(1024,kernel_size=(1,1),strides=1,padding='same',use_bias=False)\n",
    "        self.bn6 = layers.BatchNormalization()\n",
    "        self.ac7 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv8 = layers.Conv2D(512,kernel_size=(1,1),strides=1,padding='same',use_bias=False)\n",
    "        self.bn7 = layers.BatchNormalization()\n",
    "\n",
    "        self.conv9 = layers.Conv2D(128,kernel_size=(1,1),strides=1,padding='same',use_bias=False)\n",
    "        self.bn8 = layers.BatchNormalization()\n",
    "        self.ac8 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv10 = layers.Conv2D(128,kernel_size=(3,3),strides=1,padding='same',use_bias=False)\n",
    "        self.bn9 = layers.BatchNormalization()\n",
    "        self.ac9 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv11 = layers.Conv2D(512,kernel_size=(3,3),strides=1,padding='same',use_bias=False)\n",
    "        self.bn10 = layers.BatchNormalization()\n",
    "\n",
    "        self.ac10 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.conv12 = layers.Conv2D(64*8,kernel_size=1,strides=1,padding='same')\n",
    "        self.bn11 = layers.BatchNormalization()\n",
    "        self.ac11 = layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1)\n",
    "\n",
    "    def call(self,inputs,training=False):\n",
    "        x = self.ac1(self.conv1(inputs[0]))\n",
    "        x = self.ac2(self.bn1(self.conv2(x),training=training))\n",
    "        x = self.ac3(self.bn2(self.conv3(x),training=training))\n",
    "        x = self.ac4(self.bn3(self.conv4(x),training=training))\n",
    "        x = self.ac5(self.bn4(self.conv5(x),training=training))\n",
    "        x = self.ac6(self.bn5(self.conv6(x),training=training))\n",
    "        x = self.ac7(self.bn6(self.conv7(x),training=training))\n",
    "        x = self.bn7(self.conv8(x))\n",
    "        \n",
    "        x2 = self.ac8(self.bn8(self.conv9(x),training=training))\n",
    "        x2 = self.ac9(self.bn9(self.conv10(x2),training=training))\n",
    "        x2 = self.bn10(self.conv11(x2))\n",
    "\n",
    "        added_x = layers.add([x,x2])\n",
    "        added_x = self.ac10(added_x)\n",
    "\n",
    "        input_layer2 = self.e_ac(self.e_fc(inputs[1]))\n",
    "        input_layer2 = tf.reshape(input_layer2,shape=(-1,1,1,128))\n",
    "        input_layer2 = tf.tile(input_layer2,[1,4,4,1])\n",
    "        x3 = tf.concat([added_x,input_layer2],axis=-1)\n",
    "\n",
    "        x3 = self.ac11(self.bn11(self.conv12(x3),training=training))\n",
    "        x3 = self.flatten(x3)\n",
    "        x3 = self.fc(x3)\n",
    "        x3 = tf.sigmoid(x3)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def celoss_zeros(logits):\n",
    "\t# 计算属于与标签为0的交叉熵，使用标签平滑\n",
    "    y = tf.ones_like(logits) * 0.1\n",
    "    loss = keras.losses.binary_crossentropy(y,logits)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def celoss_ones(logits):\n",
    "    # 计算属于与标签为1的交叉熵，使用标签平滑\n",
    "    y = tf.ones_like(logits) * 0.9\n",
    "    loss = keras.losses.binary_crossentropy(y, logits)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def KL_loss(logits):\n",
    "    mean = logits[0]\n",
    "    logsigma = logits[1]\n",
    "    loss = -logsigma + 0.5 * (-1 + tf.exp(2. * logsigma) + tf.square(mean))\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def d_loss_fn(batch_size,generator,discriminator,img_batch,embedding_batch,z_noise,condition_var,training):\n",
    "    # 采样生成图片\n",
    "    # print('======================================================')\n",
    "    # print(embedding_batch.shape)\n",
    "    fake_images,_ = generator([embedding_batch,z_noise,condition_var],training)\n",
    "    # print('======================================================')\n",
    "    # 判定生成图片\n",
    "    d_fake_logits = discriminator([fake_images,embedding_batch], training)\n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    # 判定真实图片\n",
    "    d_real_logits = discriminator([img_batch,embedding_batch], training)\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    # 判定不符嵌入\n",
    "    d_wrong_logits = discriminator([img_batch[:(batch_size-1)],embedding_batch[1:]],training)\n",
    "    d_loss_wrong = celoss_zeros(d_wrong_logits)\n",
    "    loss = d_loss_fake + d_loss_real + d_loss_wrong\n",
    "    return loss\n",
    "\n",
    "def g_loss_fn(generator,discriminator,embedding_batch,z_noise,condition_var,training):\n",
    "    fake_images,mean_logsigma = generator([embedding_batch,z_noise,condition_var],training)\n",
    "    d_fake_logits = discriminator([fake_images,embedding_batch], training)\n",
    "    d_loss_fake = celoss_ones(d_fake_logits)\n",
    "    d_KL_fake = KL_loss(mean_logsigma)\n",
    "    loss = d_loss_fake + 2.0 * d_KL_fake\n",
    "    return loss\n",
    "\n",
    "def d_loss_fn_stage2(batch_size=64,\n",
    "                     gen_stage1=None,\n",
    "                     gen_stage2=None,\n",
    "                     dis_stage2=None,\n",
    "                     image_batch=None,\n",
    "                     embedding_batch=None,\n",
    "                     z_noise=None,\n",
    "                     condition_var=None,\n",
    "                     training=False):\n",
    "    lr_fake_images,_ = gen_stage1([embedding_batch,z_noise,condition_var])\n",
    "    hr_fake_images,_ = gen_stage2([embedding_batch,lr_fake_images,condition_var],training)\n",
    "    # 判定生成图片\n",
    "    d_fake_logits = dis_stage2([hr_fake_images,embedding_batch], training)\n",
    "    d_loss_fake = celoss_zeros(d_fake_logits)\n",
    "    # 判定真实图片\n",
    "    d_real_logits = dis_stage2([image_batch,embedding_batch], training)\n",
    "    d_loss_real = celoss_ones(d_real_logits)\n",
    "    # 判定不符嵌入\n",
    "    d_wrong_logits = dis_stage2([image_batch[:(batch_size-1)],embedding_batch[1:]],training)\n",
    "    d_loss_wrong = celoss_zeros(d_wrong_logits)\n",
    "    loss = d_loss_fake + d_loss_real + d_loss_wrong\n",
    "    return loss\n",
    "\n",
    "def g_loss_fn_stage2(gen_stage1=None,\n",
    "                     gen_stage2=None,\n",
    "                     dis_stage2=None,\n",
    "                     embedding_batch=None,\n",
    "                     z_noise=None,\n",
    "                     condition_var=None,\n",
    "                     training=False):\n",
    "    lr_fake_images,_ = gen_stage1([embedding_batch,z_noise,condition_var])\n",
    "    hr_fake_images,mean_logsigma = gen_stage2([embedding_batch,lr_fake_images,condition_var],training)\n",
    "    d_fake_logits = dis_stage2([hr_fake_images,embedding_batch], training)\n",
    "    d_loss_fake = celoss_ones(d_fake_logits)\n",
    "    d_KL_fake = KL_loss(mean_logsigma)\n",
    "    loss = d_loss_fake + 2.0 * d_KL_fake\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图片保存函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(val_out,val_block_size,image_path,color_mode):\n",
    "    def preprocessing(img):\n",
    "        img = ((img + 1.0)*(255./2)).astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    preprocessed = preprocessing(val_out)\n",
    "    final_image = np.array([])\n",
    "    single_row = np.array([])\n",
    "    for b in range(val_out.shape[0]):\n",
    "        # concat image into a row\n",
    "        if single_row.size == 0:\n",
    "            single_row = preprocessed[b,:,:,:]\n",
    "        else:\n",
    "            single_row = np.concatenate((single_row,preprocessed[b,:,:,:]),axis=1)\n",
    "        # concat image row to final_image\n",
    "        if (b+1) % val_block_size == 0:\n",
    "            if final_image.size == 0:\n",
    "                final_image = single_row\n",
    "            else:\n",
    "                final_image = np.concatenate((final_image, single_row), axis=0)\n",
    "\n",
    "            # reset single row\n",
    "            single_row = np.array([])\n",
    "\n",
    "    if final_image.shape[2] == 1:\n",
    "        final_image = np.squeeze(final_image, axis=2)\n",
    "    Image.fromarray(final_image).save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_ROW = 8\n",
    "SAMPLE_COL = 8\n",
    "SAMPLE_NUM = SAMPLE_ROW * SAMPLE_COL\n",
    "\n",
    "samples_dir = './samples/'\n",
    "if not os.path.exists(samples_dir):\n",
    "    os.makedirs(samples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1024)\n",
      "the petals of the flower are pink in color and have a yellow center \n",
      "this flower has white petals and yellow pistil as its main features \n",
      "the petals on this flower are pink with white stamen \n",
      "the flower has a smooth purple petal with white pollen tubes and yellow anther \n",
      "this white flower has bright yellow stamen with large anther \n",
      "this flower has four very broad light pink petals with a yellow green center \n",
      "this flower is white and pink in color with petals that have small veins \n",
      "the flower has petals that are soft smooth thin and separately arranged around stamens forming bowl like shape \n"
     ]
    }
   ],
   "source": [
    "sample_embeddings = []\n",
    "text_list = []\n",
    "for embeddings, text in zip(df_train['Embeddings'].values, df_train['Texts'].values):\n",
    "    for i in range(SAMPLE_ROW):\n",
    "        sample_embeddings.append(embeddings[0])\n",
    "    text_list.append(text[0])\n",
    "    if len(sample_embeddings) == SAMPLE_NUM:\n",
    "        break\n",
    "\n",
    "sample_embeddings = tf.Variable(sample_embeddings)\n",
    "print(sample_embeddings.shape)\n",
    "for i in text_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main_stage1():\n",
    "    image_size = 64\n",
    "    batch_size = 16\n",
    "    z_dim = 100\n",
    "    stage1_generator_lr = 0.0002\n",
    "    stage1_discriminator_lr = 0.0002\n",
    "    stage1_lr_decay_step = 600\n",
    "    epochs = 100\n",
    "    condition_dim = 128\n",
    "    training=True\n",
    "\n",
    "    d_optimizer = keras.optimizers.Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "    g_optimizer = keras.optimizers.Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "\n",
    "    # db_train = dataset_generator('./dataset/text2ImgData.pkl', batch_size, training_data_generator)\n",
    "    db_train = training_dataset_generator(df_train['ImagePath'], df_train['Embeddings'], batch_size)\n",
    "\n",
    "\n",
    "    \n",
    "    gen = Generator_stage1()\n",
    "    gen.build([[4,1024],[4,100],[128]]) # [embedding_batch,z_noise,condition_var]\n",
    "    try:\n",
    "        gen.load_weights(\"stage1_gen.h5\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    dis = Discriminator_stage1()\n",
    "    dis.build([[4,64,64,3],[4,1024]])\n",
    "    try:\n",
    "        dis.load_weights(\"stage1_dis.h5\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    #real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
    "    #fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
    "    for epoch in range(epochs):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        for index,(x,embedding) in enumerate(db_train):\n",
    "            z_noise = tf.random.normal(shape=(batch_size,z_dim))\n",
    "            condition_var = tf.random.normal(shape=(condition_dim,))\n",
    "            with tf.GradientTape() as tape:\n",
    "                d_loss = d_loss_fn(batch_size,gen,dis,x,embedding,z_noise,condition_var,training)\n",
    "            grads = tape.gradient(d_loss,dis.trainable_variables)\n",
    "            d_optimizer.apply_gradients(zip(grads,dis.trainable_variables))\n",
    "            z_noise = tf.random.normal(shape=(batch_size,z_dim))\n",
    "            condition_var = tf.random.normal(shape=(condition_dim,))\n",
    "            with tf.GradientTape() as tape:\n",
    "                g_loss = g_loss_fn(gen,dis,embedding,z_noise,condition_var,training)\n",
    "            grads = tape.gradient(g_loss,gen.trainable_variables)\n",
    "            g_optimizer.apply_gradients(zip(grads,gen.trainable_variables))\n",
    "            # print(f'batch: {index}  // d_loss: {d_loss}  // g_loss: {g_loss}')\n",
    "        if epoch % 1 == 0:\n",
    "            z = tf.random.normal([64,z_dim])\n",
    "            print(epoch,'d_loss:',float(d_loss),'g_loss:',float(g_loss))\n",
    "            #可视化\n",
    "            condition_var = tf.random.normal(shape=(condition_dim,))\n",
    "            fake_image,_ = gen([sample_embeddings,z,condition_var],training=False)\n",
    "            timestamp = int(time.time())\n",
    "            img_path = f\"testout/ganstage1_{epoch}_{timestamp}.png\"\n",
    "            save_result(fake_image.numpy(),8,img_path,color_mode='P')\n",
    "            d_losses.append(float(d_loss))\n",
    "            g_losses.append(float(g_loss))\n",
    "        if epoch % 1 == 0:\n",
    "            timestamp = int(time.time())\n",
    "            gen.save_weights(f\"./weight/stage1_gen_{epoch}_{timestamp}.h5\")\n",
    "            dis.save_weights(f\"./weight/stage1_dis_{epoch}_{timestamp}.h5\")\n",
    "\n",
    "def main_stage2():\n",
    "    image_size = 256\n",
    "    batch_size = 16\n",
    "    z_dim = 100\n",
    "    stage1_generator_lr = 0.0002\n",
    "    stage1_discriminator_lr = 0.0002\n",
    "    stage1_lr_decay_step = 600\n",
    "    epochs = 100\n",
    "    condition_dim = 128\n",
    "    training=True\n",
    "\n",
    "\n",
    "\n",
    "    d_optimizer = keras.optimizers.Adam(lr=stage1_discriminator_lr, beta_1=0.5, beta_2=0.999)\n",
    "    g_optimizer = keras.optimizers.Adam(lr=stage1_generator_lr, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    #Load dataset\n",
    "    # db_hr_train = H_dataset_generator('./dataset/text2ImgData.pkl', batch_size, H_training_data_generator)\n",
    "    db_hr_train = stage2_training_dataset_generator(df_train['ImagePath'], df_train['Embeddings'], batch_size)\n",
    "    \n",
    "    # db_hr_test = testing_dataset_generator(8, testing_data_generator)\n",
    "    # db_hr_test = iter(db_hr_test)\n",
    "    # embeddings_test = next(db_hr_test)\n",
    "    # embeddings_test = np.repeat(embeddings_test, 8, axis=0)\n",
    "    \n",
    "    gen_stage1 = Generator_stage1()\n",
    "    gen_stage1.build([[4,1024],[4,100],[128]])\n",
    "    try:\n",
    "        gen_stage1.load_weights(\"weight/stage1_gen_20_1702883747.h5\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    gen_stage2 = Generator_stage2()\n",
    "    gen_stage2.build([[4,1024],[4,64,64,3],[128]])\n",
    "    try:\n",
    "        gen_stage2.load_weights(\"weight/stage2_gen_20_1709.h5\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    dis_stage2 = Discriminator_stage2()\n",
    "    dis_stage2.build([[4,256,256,3],[4,1024]])\n",
    "    try:\n",
    "        dis_stage2.load_weights(\"weight/stage2_dis_20_17069.h5\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        for index,(x,embedding) in enumerate(db_hr_train):\n",
    "            z_noise = tf.random.normal(shape=(batch_size,z_dim))\n",
    "            condition_var = tf.random.normal(shape=(condition_dim,))\n",
    "            with tf.GradientTape() as tape:\n",
    "                d_loss = d_loss_fn_stage2(batch_size=batch_size,\n",
    "                                          gen_stage1=gen_stage1,\n",
    "                                          gen_stage2=gen_stage2,\n",
    "                                          dis_stage2=dis_stage2,\n",
    "                                          image_batch=x,\n",
    "                                          embedding_batch=embedding,\n",
    "                                          z_noise=z_noise,\n",
    "                                          condition_var=condition_var,\n",
    "                                          training=training)\n",
    "            grads = tape.gradient(d_loss,dis_stage2.trainable_variables)\n",
    "            d_optimizer.apply_gradients(zip(grads,dis_stage2.trainable_variables))\n",
    "            z_noise = tf.random.normal(shape=(batch_size,z_dim))\n",
    "            condition_var = tf.random.normal(shape=(condition_dim,))\n",
    "            with tf.GradientTape() as tape:\n",
    "                g_loss = g_loss_fn_stage2(gen_stage1=gen_stage1,\n",
    "                                          gen_stage2=gen_stage2,\n",
    "                                          dis_stage2=dis_stage2,\n",
    "                                          embedding_batch=embedding,\n",
    "                                          z_noise=z_noise,\n",
    "                                          condition_var=condition_var,\n",
    "                                          training=training)\n",
    "            grads = tape.gradient(g_loss,gen_stage2.trainable_variables)\n",
    "            g_optimizer.apply_gradients(zip(grads,gen_stage2.trainable_variables))\n",
    "            # print(f'batch: {index}  // d_loss: {d_loss}  // g_loss: {g_loss}')\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch,'d_loss:',float(d_loss),'g_loss:',float(g_loss))\n",
    "            #可视化\n",
    "            z = tf.random.normal([64,z_dim])\n",
    "            condition_var = tf.random.normal(shape=(condition_dim,))\n",
    "            lr_fake_image,_ = gen_stage1([sample_embeddings,z,condition_var],training=False)\n",
    "            hr_fake_image,_ = gen_stage2([sample_embeddings,lr_fake_image,condition_var],training=False)\n",
    "            timestamp = int(time.time())\n",
    "            img_path = f\"testout/ganstage2_{epoch}_{timestamp}.png\"\n",
    "            save_result(hr_fake_image.numpy(),8,img_path,color_mode='P')\n",
    "            d_losses.append(float(d_loss))\n",
    "            g_losses.append(float(g_loss))\n",
    "        if epoch % 1 == 0:\n",
    "            timestamp = int(time.time())\n",
    "            gen_stage2.save_weights(f\"./weight/stage2_gen_{epoch}_{timestamp}.h5\")\n",
    "            dis_stage2.save_weights(f\"./weight/stage2_dis_{epoch}_{timestamp}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s111062697/miniconda3/envs/eric/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] Unable to open file (unable to open file: name = 'stage1_gen.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to open file (unable to open file: name = 'stage1_dis.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 15:02:01.344687: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-18 15:02:01.453054: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 d_loss: 1.5703572034835815 g_loss: 1.3039648532867432\n",
      "2 d_loss: 1.7273536920547485 g_loss: 1.0726125240325928\n",
      "4 d_loss: 1.6657315492630005 g_loss: 1.6929434537887573\n",
      "6 d_loss: 1.5841612815856934 g_loss: 1.3345985412597656\n",
      "8 d_loss: 1.5701602697372437 g_loss: 1.282297134399414\n",
      "10 d_loss: 1.361458420753479 g_loss: 1.575119137763977\n",
      "12 d_loss: 1.3865246772766113 g_loss: 1.2897465229034424\n",
      "14 d_loss: 1.6156845092773438 g_loss: 1.547042727470398\n",
      "16 d_loss: 1.3441431522369385 g_loss: 1.2917463779449463\n",
      "18 d_loss: 1.467040777206421 g_loss: 0.8157751560211182\n",
      "20 d_loss: 1.1515706777572632 g_loss: 2.284703493118286\n",
      "22 d_loss: 1.9683808088302612 g_loss: 2.9307775497436523\n",
      "24 d_loss: 1.4202892780303955 g_loss: 1.5177223682403564\n",
      "26 d_loss: 1.2172489166259766 g_loss: 2.498117208480835\n",
      "28 d_loss: 1.2232774496078491 g_loss: 2.6428186893463135\n",
      "30 d_loss: 1.3394407033920288 g_loss: 2.691617488861084\n",
      "32 d_loss: 1.0766499042510986 g_loss: 1.845373511314392\n",
      "34 d_loss: 1.108124852180481 g_loss: 2.9342424869537354\n",
      "36 d_loss: 1.2127587795257568 g_loss: 1.7387704849243164\n",
      "38 d_loss: 1.2179293632507324 g_loss: 1.4674521684646606\n",
      "40 d_loss: 1.0732736587524414 g_loss: 1.6488525867462158\n",
      "42 d_loss: 1.1291091442108154 g_loss: 2.009877920150757\n",
      "44 d_loss: 1.2234231233596802 g_loss: 2.141510248184204\n",
      "46 d_loss: 1.0877853631973267 g_loss: 2.4416584968566895\n",
      "48 d_loss: 1.0730185508728027 g_loss: 2.3571882247924805\n",
      "50 d_loss: 1.0400927066802979 g_loss: 1.2363009452819824\n",
      "52 d_loss: 1.100058674812317 g_loss: 2.6998300552368164\n",
      "54 d_loss: 1.0513285398483276 g_loss: 1.376465082168579\n",
      "56 d_loss: 1.0875343084335327 g_loss: 2.8379077911376953\n",
      "58 d_loss: 1.164023518562317 g_loss: 2.7871150970458984\n",
      "60 d_loss: 1.0130023956298828 g_loss: 2.6949241161346436\n",
      "62 d_loss: 1.0369460582733154 g_loss: 2.8838143348693848\n",
      "64 d_loss: 1.0400185585021973 g_loss: 2.735882520675659\n",
      "66 d_loss: 1.0321476459503174 g_loss: 1.921248435974121\n",
      "68 d_loss: 1.1531516313552856 g_loss: 2.4971742630004883\n",
      "70 d_loss: 1.1111524105072021 g_loss: 2.493778705596924\n",
      "72 d_loss: 1.0324938297271729 g_loss: 2.821850299835205\n",
      "74 d_loss: 1.1710829734802246 g_loss: 2.160146951675415\n",
      "76 d_loss: 1.084989070892334 g_loss: 2.5794870853424072\n",
      "78 d_loss: 1.0761289596557617 g_loss: 2.173567056655884\n",
      "80 d_loss: 1.2381377220153809 g_loss: 3.610262155532837\n",
      "82 d_loss: 1.085637092590332 g_loss: 2.8345394134521484\n",
      "84 d_loss: 1.0226672887802124 g_loss: 2.839055299758911\n",
      "86 d_loss: 1.0179986953735352 g_loss: 2.521235227584839\n",
      "88 d_loss: 1.0219008922576904 g_loss: 1.702674150466919\n",
      "90 d_loss: 1.072273850440979 g_loss: 1.927130937576294\n",
      "92 d_loss: 1.0232552289962769 g_loss: 1.5368621349334717\n",
      "94 d_loss: 1.4503350257873535 g_loss: 2.175788164138794\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain_stage1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mmain_stage1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     45\u001b[0m     d_loss \u001b[38;5;241m=\u001b[39m d_loss_fn(batch_size,gen,dis,x,embedding,z_noise,condition_var,training)\n\u001b[0;32m---> 46\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads,dis\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     48\u001b[0m z_noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m(batch_size,z_dim))\n",
      "File \u001b[0;32m~/miniconda3/envs/eric/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1107\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1108\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1109\u001b[0m           output_gradients))\n\u001b[1;32m   1110\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1111\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1113\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/miniconda3/envs/eric/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/eric/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:689\u001b[0m, in \u001b[0;36m_zeros\u001b[0;34m(shape, dtype)\u001b[0m\n\u001b[1;32m    685\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39mzeros(shape, dtype)\n\u001b[1;32m    687\u001b[0m device \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mdevice_name\n\u001b[0;32m--> 689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtensor_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_tf_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    690\u001b[0m   shape_key \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mref()\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/eric/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py:1059\u001b[0m, in \u001b[0;36mis_tf_type\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_tf_type\u001b[39m(x):  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m   \u001b[38;5;124;03m\"\"\"Checks whether `x` is a TF-native type that can be passed to many TF ops.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m  Use `is_tensor` to differentiate types that can ingested by TensorFlow ops\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `True` if `x` is a TensorFlow-native type.\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1059\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNativeObject\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m           \u001b[38;5;28misinstance\u001b[39m(x, core\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m           \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_tensor_like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_stage1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] Unable to open file (unable to open file: name = 'weight/stage2_gen_20_1709.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to open file (unable to open file: name = 'weight/stage2_dis_20_17069.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "main_stage2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
