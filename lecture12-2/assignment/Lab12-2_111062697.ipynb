{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Image Captioning <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "111062697 吳律穎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "1. You can use any model architectures (or any code above) you want, as long as accomplishing the goal.\n",
    "2. You should train your own model architecture. In other words, except the feature extractor part, do not load the model or any pre-trained weights directly from other sources.\n",
    "    - You can use pretrained inception_v3 as feature extractor; however, it's not recommended since inception is pretrained on ImageNet, where the image pattern is quite different to English words.\n",
    "3. You should use the first 100,000 images as training data, the next 20,000 as validation data, and the rest (final 20,000) as testing data.\n",
    "    - `spec_train_val`.txt contains the labels of only first 120,000 images.\n",
    "4. Only if the whole word matches exactly does it count as correct.\n",
    "5. You need to predict the answer to the testing data and write them in a file.\n",
    "6. Your testing accuracy should be at least 90% in validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "txt_path = './dataset/words_captcha/spec_train_val.txt'\n",
    "with open(txt_path, 'r') as fin:\n",
    "    for line in fin:\n",
    "        image_name, caption = line.strip().split()\n",
    "        all_captions.append('<start> ' + ' '.join(caption) + ' <end>')\n",
    "        all_img_name_vector.append(f'./dataset/words_captcha/{image_name}.png')\n",
    "        \n",
    "train_captions, img_name_vector = shuffle(all_captions, all_img_name_vector, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val = img_name_vector[:100000], img_name_vector[100000:]\n",
    "cap_train, cap_val = train_captions[:100000], train_captions[100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_test = []\n",
    "for i in range(120000, 140000):\n",
    "    img_name_test.append(f'./dataset/words_captcha/a{i}.png')\n",
    "# img_name_test = shuffle(img_name_test, random_state=1)\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(img_name_test[i])\n",
    "# print('------------------------------------------------------')\n",
    "# for i in range(5):\n",
    "#     print(img_name_train[i])\n",
    "# print('------------------------------------------------------')\n",
    "# for i in range(5):\n",
    "#     print(img_name_val[i])\n",
    "# print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20000, 20000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_name_train), len(img_name_val), len(img_name_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i r a q <end>\n",
      "<start> i r a q <end>\n"
     ]
    }
   ],
   "source": [
    "print(train_captions[0])\n",
    "print(cap_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\" \", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "cap_train = tokenizer.texts_to_sequences(cap_train)\n",
    "cap_train = tf.keras.preprocessing.sequence.pad_sequences(cap_train, padding='post')\n",
    "\n",
    "cap_val = tokenizer.texts_to_sequences(cap_val)\n",
    "cap_val = tf.keras.preprocessing.sequence.pad_sequences(cap_val, padding='post')\n",
    "\n",
    "max_length_test = calc_max_length(cap_val)\n",
    "max_length_train = calc_max_length(cap_train)\n",
    "max_length = max(max_length_test, max_length_train)\n",
    "print(max_length_train)\n",
    "print(max_length_test)\n",
    "\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = 5000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "STEPS = len(img_name_train) // BATCH_SIZE\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img = tf.io.read_file(img_name)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, (244, 244))\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restnet50 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "feature_extractor = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape after passing through fc == (batch_size, 15, embedding_dim)\n",
    "        x = self.feature_extractor(x)\n",
    "        x = tf.reshape(x, (x.shape[0], -1, x.shape[3]))\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\")\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoints/Resnet50V2/'\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    mean_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length-1): #前面有start了，所以只需max_length-1\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(segs):\n",
    "    result_list = []\n",
    "    for seq in segs:\n",
    "        result = ''\n",
    "        for s in seq[1:]:\n",
    "            if s == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            result += tokenizer.index_word[s]\n",
    "        result_list.append(result)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset_valid):\n",
    "    sample_count = 0\n",
    "    correct_count = 0\n",
    "    for img_tensor, target in dataset_valid:\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        real_list = postprocess(target.numpy())\n",
    "        for pred, real in zip(pred_list, real_list):\n",
    "            sample_count += 1\n",
    "            if pred == real:\n",
    "                correct_count += 1\n",
    "\n",
    "    return correct_count / sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 2000/2000 [11:25<00:00,  2.92it/s, loss=0.609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 2000/2000 [12:07<00:00,  2.75it/s, loss=0.0291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 2000/2000 [12:30<00:00,  2.66it/s, loss=0.0189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 2000/2000 [12:55<00:00,  2.58it/s, loss=0.0165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 2000/2000 [12:55<00:00,  2.58it/s, loss=0.0131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 2000/2000 [13:29<00:00,  2.47it/s, loss=0.0126] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 2000/2000 [13:47<00:00,  2.42it/s, loss=0.00721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 2000/2000 [13:57<00:00,  2.39it/s, loss=0.00822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 2000/2000 [13:58<00:00,  2.39it/s, loss=0.00698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 2000/2000 [13:59<00:00,  2.38it/s, loss=0.00576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.98\n",
      "Time taken for 10 epoch 8788.310842752457 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "loss_plot = []\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    loss = 0\n",
    "    pbar = tqdm(dataset_train, total=STEPS, desc=f'Epoch {epoch + 1:2d}')\n",
    "    for (step, (img_tensor, target)) in enumerate(pbar):\n",
    "        loss += train_step(img_tensor, target)\n",
    "        pbar.set_postfix({'loss': loss.numpy() / (step + 1)})\n",
    "\n",
    "    loss_plot.append(loss / STEPS)\n",
    "    ckpt_manager.save()\n",
    "\n",
    "    score = evaluate(dataset_val)\n",
    "    print(f'Validation accuracy: {score:.2f}')\n",
    "    \n",
    "print('Time taken for {} epoch {} sec\\n'.format(EPOCHS - start_epoch, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x23168441990>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.restore('./checkpoints/Resnet50V2/ckpt-9').expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func_test(img_name):\n",
    "    img = tf.io.read_file(img_name)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, (244, 244))\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    return img, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((img_name_test))\\\n",
    "                               .map(map_func_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:23<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('./Lab12-2_111062697.txt', 'w') as fout:\n",
    "    for step, (img_tensor, img_name) in enumerate(tqdm(dataset_test)):\n",
    "        pred_list = postprocess(predict(img_tensor).numpy())\n",
    "        for path, pred in zip(img_name, pred_list):\n",
    "            path = path.numpy().decode('utf-8')\n",
    "            name = re.search('(a[0-9]+)', path).group(1)\n",
    "            fout.write(f'{name} {pred}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我以 ResNet50 作為 feature extractor，並使用 pretrained on ImageNet 的 weight，來當 pretrain weight，其餘的部分則與助教的相同。由於看到 Requirements 中說 ImageNet 作為 pretrain weight 效果不好，所以將 ResNet50 的 trainable_variables 也納入需要訓練並更新的參數中。從訓練結果可以看到，在第一個 Epoch 就有 0.95 的 Validation accuracy，第五個 Epoch 便達到 0.99 ，最後我選擇了同樣是 0.99 的第十個 Epoch 來產生 testset 的結果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
