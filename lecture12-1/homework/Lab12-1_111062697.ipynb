{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**Seq2Seq Learning & Neural Machine Translation**<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "111062697 吳律穎\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will train a seq2seq model with **Luong Attention** to solve a sentiment analysis task with the IMDB dataset. The formula of score function in Luong Attention is:"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "這個數據集包含50,000個帶有二進制標籤（正面和負面）的句子。在這裡，我們將數據拆分為訓練集和測試集。值得一提的是，與神經機器翻譯不同，用於情感分析的解碼器是4個全連接層，而不是GRU層，因為在這裡我們想進行二元分類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "movie_reviews = pd.read_csv(\"./dataset/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any null value in the dataset\n",
    "movie_reviews.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the size of the dataset\n",
    "movie_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[\"review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "# replace the positive with 1, replace the negative with 0\n",
    "y = movie_reviews['sentiment']\n",
    "y = np.array([1 if x == \"positive\" else 0 for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training data: 40000\n",
      "# test data: 10000\n"
     ]
    }
   ],
   "source": [
    "# Split the training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "# padding sentences to the same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  296,  140, 2854,    2,  405,  614,    1,  263,    5, 3514,\n",
       "        977,    4,   25,   37,   11, 1237,  215,   62,    2,   35,    5,\n",
       "         27,  217,   24,  189, 1430,    7, 1068,   15, 4868,   81,    1,\n",
       "        221,   63,  351,   64,   52,   24,    4, 3547,   13,    6,   19,\n",
       "        192,    4, 8148,  859, 3430, 1720,   17,   23,    4,  158,  194,\n",
       "        175,  106,    9, 1604,  461,   71,  218,    4,  321,    2, 3431,\n",
       "         31,   20,   47,   68, 1844, 4668,   11,    6, 1365,    8,   16,\n",
       "          5, 3475, 1990,   14,   59,    1, 2380,  460,  518,    2,  170,\n",
       "       2524, 2698, 1745,    4,  573,    6,   33,    1, 3750,  198,  345,\n",
       "       3812])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the preprocessed data\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 100]), TensorShape([128]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# only reserve 10000 words\n",
    "vocab_size = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x is the training data with shape == (batch_size，max_length)  -> (128, 100)\n",
    "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
    "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
    "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
    "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
    "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (128, 100, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (128, 1024)\n",
      "tf.Tensor([ True  True  True ...  True  True  True], shape=(1024,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# the output and the hidden state of GRU is equal\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        # TODO: Complete the function.\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # TODO: Implement the Luong attention.\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = tf.matmul(self.W(values), tf.transpose(hidden_with_time_axis, perm=[0, 2, 1]))  # shape 才對\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # pass through four fully connected layers, the model will return \n",
    "        # the probability of the positivity of the sentence\n",
    "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
    "        self.fc_2 = tf.keras.layers.Dense(512)\n",
    "        self.fc_3 = tf.keras.layers.Dense(64)\n",
    "        self.fc_4 = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        output = self.fc_1(context_vector)\n",
    "        output = self.fc_2(output)\n",
    "        output = self.fc_3(output)\n",
    "        output = self.fc_4(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 1)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
    "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./checkpoints/sentiment-analysis\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, _ = decoder(enc_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(targ, predictions)\n",
    "\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.6931\n",
      "Epoch 1 Batch 100 Loss 0.3956\n",
      "Epoch 1 Batch 200 Loss 0.2849\n",
      "Epoch 1 Batch 300 Loss 0.2220\n",
      "Epoch 1 Loss 0.3796\n",
      "Time taken for 1 epoch 24.855294466018677 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2159\n",
      "Epoch 2 Batch 100 Loss 0.2222\n",
      "Epoch 2 Batch 200 Loss 0.2392\n",
      "Epoch 2 Batch 300 Loss 0.2432\n",
      "Epoch 2 Loss 0.2539\n",
      "Time taken for 1 epoch 22.599584102630615 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1371\n",
      "Epoch 3 Batch 100 Loss 0.1330\n",
      "Epoch 3 Batch 200 Loss 0.2867\n",
      "Epoch 3 Batch 300 Loss 0.2806\n",
      "Epoch 3 Loss 0.1847\n",
      "Time taken for 1 epoch 22.098793745040894 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0548\n",
      "Epoch 4 Batch 100 Loss 0.0870\n",
      "Epoch 4 Batch 200 Loss 0.1569\n",
      "Epoch 4 Batch 300 Loss 0.1730\n",
      "Epoch 4 Loss 0.1227\n",
      "Time taken for 1 epoch 22.759811401367188 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1016\n",
      "Epoch 5 Batch 100 Loss 0.0728\n",
      "Epoch 5 Batch 200 Loss 0.0497\n",
      "Epoch 5 Batch 300 Loss 0.1311\n",
      "Epoch 5 Loss 0.0808\n",
      "Time taken for 1 epoch 22.26614999771118 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0386\n",
      "Epoch 6 Batch 100 Loss 0.0255\n",
      "Epoch 6 Batch 200 Loss 0.0332\n",
      "Epoch 6 Batch 300 Loss 0.0658\n",
      "Epoch 6 Loss 0.0534\n",
      "Time taken for 1 epoch 22.720097064971924 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0321\n",
      "Epoch 7 Batch 100 Loss 0.0346\n",
      "Epoch 7 Batch 200 Loss 0.0263\n",
      "Epoch 7 Batch 300 Loss 0.0681\n",
      "Epoch 7 Loss 0.0436\n",
      "Time taken for 1 epoch 22.244836807250977 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0141\n",
      "Epoch 8 Batch 100 Loss 0.0196\n",
      "Epoch 8 Batch 200 Loss 0.0620\n",
      "Epoch 8 Batch 300 Loss 0.0123\n",
      "Epoch 8 Loss 0.0347\n",
      "Time taken for 1 epoch 22.736170053482056 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0130\n",
      "Epoch 9 Batch 100 Loss 0.0371\n",
      "Epoch 9 Batch 200 Loss 0.0537\n",
      "Epoch 9 Batch 300 Loss 0.0435\n",
      "Epoch 9 Loss 0.0293\n",
      "Time taken for 1 epoch 22.275832891464233 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0464\n",
      "Epoch 10 Batch 100 Loss 0.0188\n",
      "Epoch 10 Batch 200 Loss 0.0457\n",
      "Epoch 10 Batch 300 Loss 0.0211\n",
      "Epoch 10 Loss 0.0239\n",
      "Time taken for 1 epoch 22.74376106262207 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the epochs for training\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # get the initial hidden state of gru\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (inp, targ) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print(\"Epoch {} Loss {:.4f}\".format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print(\"Time taken for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/sentiment-analysis\\ckpt-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2f923056a10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(inp, enc_hidden):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for batch, (inp, targ) in enumerate(test_data):\n",
    "        if len(inp) != BATCH_SIZE:\n",
    "            enc_hidden = tf.zeros((len(inp), units))\n",
    "        # make prediction\n",
    "        if batch == 0:\n",
    "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
    "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
    "        else:\n",
    "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
    "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
    "            predictions = np.concatenate((predictions, _predictions))\n",
    "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
    "    \n",
    "    predictions = np.squeeze(predictions)\n",
    "    attention_weights = np.squeeze(attention_weights)\n",
    "    predictions[np.where(predictions < 0.5)] = 0\n",
    "    predictions[np.where(predictions >= 0.5)] = 1\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, attention_weights = evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8459\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', (y_pred == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:  0.8458\n",
    "We reach ~84.5% accuracy with only 10 epochs! Not bad at all! Besides the nice accuracy, let's try to do some more fascinating things. How about visualizing our results?\n",
    "\n",
    "In addition to the better performance, another advantage of the attention mechanism is we can visualize the attention weights. Here we demonstrate which word the model focuses on the most by coloring the words corresponding to the ten largest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 1\n",
      "y_predict: 0\n",
      "changed it was terrible main event just like every match is \u001b[31min\u001b[0m is terrible other matches \u001b[31mon\u001b[0m \u001b[31mthe\u001b[0m card were \u001b[31mrazor\u001b[0m \u001b[31mramon\u001b[0m \u001b[31mvs\u001b[0m ted brothers vs bodies shawn michaels vs this was the event where shawn named his big monster of body guard vs kid hart first takes on then takes on \u001b[31mjerry\u001b[0m and stuff with the and was always very interesting then destroyed marty undertaker took on giant in another terrible match the smoking and took on bam bam and \u001b[31mthe\u001b[0m \u001b[31mand\u001b[0m \u001b[31mthe\u001b[0m world title against lex this match was boring and it has terrible ending however it deserves \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "of subject matter as are and broken in many ways on many many issues happened to see the pilot premiere in passing and \u001b[31mjust\u001b[0m had to keep in after that to see if \u001b[31mwould\u001b[0m ever get the girl after seeing them all on television was delighted to see them available \u001b[31mon\u001b[0m dvd have to \u001b[31madmit\u001b[0m \u001b[31mthat\u001b[0m it was the \u001b[31monly\u001b[0m thing that kept me sane whilst had to do hour night shift and developed insomnia farscape was the \u001b[31monly\u001b[0m thing to get me through those extremely long nights do yourself favour \u001b[31mwatch\u001b[0m the pilot and see \u001b[31mwhat\u001b[0m \u001b[31mmean\u001b[0m farscape comet \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "destruction the first really bad thing is the guy steven seagal \u001b[31mwould\u001b[0m have been beaten to pulp by seagal \u001b[31mdriving\u001b[0m but that probably would have ended the whole premise for the movie it seems like they decided to make all kinds \u001b[31mof\u001b[0m \u001b[31mchanges\u001b[0m in the movie plot so \u001b[31mjust\u001b[0m plan to enjoy the action and do not expect coherent plot turn any sense of \u001b[31mlogic\u001b[0m you may have it will your \u001b[31mchance\u001b[0m of getting headache does give \u001b[31mme\u001b[0m some hope that steven seagal is trying to move back towards the type of characters \u001b[31mhe\u001b[0m \u001b[31mportrayed\u001b[0m in his more popular movies \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "jane austen would definitely of this one paltrow does an awesome job \u001b[31mcapturing\u001b[0m the attitude of emma she is funny without being silly yet elegant she puts on very convincing british accent not being british myself maybe m not the best \u001b[31mjudge\u001b[0m \u001b[31mbut\u001b[0m she fooled me she was also excellent in doors sometimes forget she american also brilliant are \u001b[31mjeremy\u001b[0m \u001b[31mnortham\u001b[0m and sophie \u001b[31mthompson\u001b[0m \u001b[31mand\u001b[0m law emma \u001b[31mthompson\u001b[0m sister and mother as the bates women they nearly steal the show and ms law doesn even have any lines \u001b[31mhighly\u001b[0m \u001b[31mrecommended\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "reaches the point where \u001b[31mthey\u001b[0m become obnoxious and simply frustrating touch football puzzle family and talent shows are \u001b[31mnot\u001b[0m how \u001b[31mactual\u001b[0m \u001b[31mpeople\u001b[0m \u001b[31mbehave\u001b[0m it almost sickening \u001b[31manother\u001b[0m \u001b[31mbig\u001b[0m flaw is the woman carell is supposed to be \u001b[31mfalling\u001b[0m \u001b[31mfor\u001b[0m her in her first scene with steve carell is like watching stroke victim trying to be what imagine is supposed to be unique and original in this woman comes off as \u001b[31mmildly\u001b[0m retarded it makes me think that this movie is taking place on another planet left the theater wondering what just saw after thinking further don think it was much \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "the \u001b[31mpace\u001b[0m \u001b[31mquick\u001b[0m and energetic \u001b[31mbut\u001b[0m \u001b[31mmost\u001b[0m importantly he knows how to make comedy funny he doesn the jokes and he understands that funny actors \u001b[31mknow\u001b[0m what they re doing and he allows them to do it but segal goes step further he gives tommy boy friendly almost nostalgic \u001b[31mtone\u001b[0m that both the genuinely and the critics \u001b[31mdidn\u001b[0m \u001b[31mlike\u001b[0m tommy \u001b[31mboy\u001b[0m shame on them movie doesn have to be super sophisticated or intellectual to be funny god \u001b[31mfarley\u001b[0m and spade were forced to do muted comedy la the office this is great movie and one of my all time favorites \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "for once story of hope over the tragic reality our youth face rising draws one into \u001b[31mscary\u001b[0m and unfair world and shows through beautiful color and \u001b[31mmoving\u001b[0m music how one man and his dedicated friends choose not to accept that world and change it through action and art an entertaining \u001b[31minteresting\u001b[0m emotional \u001b[31mbeautiful\u001b[0m film \u001b[31mshowed\u001b[0m \u001b[31mthis\u001b[0m \u001b[31mfilm\u001b[0m \u001b[31mto\u001b[0m numerous high school students \u001b[31mas\u001b[0m well who all \u001b[31mlive\u001b[0m in with poverty and and gun violence and they were with anderson the protagonist recommend this film to all ages over due to subtitles and some images of death from all backgrounds \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "people and sleeping around that he kept secret \u001b[31mfrom\u001b[0m most people he feels free to have an \u001b[31maffair\u001b[0m with quasi because he kevin he figures out that he can fool some people with cards like hotel but it won get him out of those the of heaven are keeping track of him and \u001b[31meverything\u001b[0m \u001b[31mhe\u001b[0m does after reading all the theories on though it seems \u001b[31mlike\u001b[0m identity is reminder of the different \u001b[31mpaths\u001b[0m \u001b[31mtony\u001b[0m could \u001b[31mve\u001b[0m \u001b[31mtaken\u001b[0m \u001b[31min\u001b[0m his life possibly along with the car joke involving that made no sense to me otherwise at that point my brain out \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "over again can remember how many times he \u001b[31msaid\u001b[0m the universe is made out of tiny little strings it like they were trying to us into just accepting are the best thing since bread finally the show ended \u001b[31moff\u001b[0m \u001b[31mwith\u001b[0m an unpleasant sense of competition between and clearly biased towards this is supposed to be an \u001b[31meducational\u001b[0m program about quantum physics not about whether the us is better \u001b[31mthan\u001b[0m \u001b[31meurope\u001b[0m or vice versa also felt \u001b[31mthat\u001b[0m \u001b[31mwas\u001b[0m part of the audiences need to see some \u001b[31mconflict\u001b[0m to remain interested please give me little more credit \u001b[31mthan\u001b[0m that overall thumbs down \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "the scenes involving joe \u001b[31mcharacter\u001b[0m in particular the scenes in the terribly clich but still funny rich but \u001b[31mscrewed\u001b[0m up characters house \u001b[31mwhere\u001b[0m the \u001b[31mstory\u001b[0m towards it final moments can see \u001b[31mhow\u001b[0m was great \u001b[31mstage\u001b[0m play and while the film makers did their best to translate this to celluloid it simply didn work and \u001b[31mwhile\u001b[0m laughed out loud at some of scenes and one liners think the \u001b[31mfirst\u001b[0m minutes \u001b[31mmy\u001b[0m senses and expectations to such degree would have laughed at anything \u001b[31munless\u001b[0m you re stuck for novelty coffee coaster don pick this up if you see it in bargain bucket \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "for idx, data in enumerate(X_test[:10]):\n",
    "    print('y_true: {:d}'.format(y_test[idx]))\n",
    "    print('y_predict: {:.0f}'.format(y_pred[idx]))\n",
    "    \n",
    "    # get the twenty most largest attention weights\n",
    "    large_weights_idx = np.argsort(attention_weights[idx])[::-1][:10]\n",
    "    \n",
    "    for _idx in range(len(data)):\n",
    "        word_idx = data[_idx]\n",
    "        if word_idx != 0:\n",
    "            if _idx in large_weights_idx:\n",
    "                print(colored(tokenizer.index_word[word_idx], 'red'), end=' ')\n",
    "                # try this if termcolor is not working properly\n",
    "                # print(f'\\033[31m{tokenizer.index_word[word_idx]}\\033[0m', end=' ')\n",
    "            else:\n",
    "                print(tokenizer.index_word[word_idx], end=' ')\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
