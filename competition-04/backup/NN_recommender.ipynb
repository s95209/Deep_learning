{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 22:13:20.959583: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 22:13:22.044251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.044374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.048550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.048660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.048751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.048911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.049777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-13 22:13:22.050640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.050749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.050843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.634485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.634624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.634722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-13 22:13:22.634797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8711 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Select GPU number 1\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderModel(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_items, embedding_size=50, hidden_units=128, learning_rate=1e-4):\n",
    "        super(RecommenderModel, self).__init__()\n",
    "        self.optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        self.user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size, input_length=1)\n",
    "        self.item_embedding = Embedding(input_dim=num_items, output_dim=embedding_size, input_length=1)\n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        self.hidden_layer = Dense(hidden_units, activation='relu')\n",
    "        self.output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_input, item_input = inputs\n",
    "        user_embedded = self.user_embedding(user_input)\n",
    "        item_embedded = self.item_embedding(item_input)\n",
    "        user_flat = self.flatten(user_embedded)\n",
    "        item_flat = self.flatten(item_embedded)\n",
    "        merged = self.concat([user_flat, item_flat])\n",
    "        hidden = self.hidden_layer(merged)\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        '''\n",
    "        Compute the MSE loss of the model\n",
    "        '''\n",
    "        loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function  # the function decorated by tf.function will be compiled into a callable TensorFlow graph automatically. This allows the TensorFlow runtime to apply optimizations and exploit parallelism to boost computation performance.\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # training=True is only needed if there are layers with different\n",
    "            # behavior during training versus inference (e.g. Dropout).\n",
    "\n",
    "            user_ids = tf.cast(data[:, 0], dtype=tf.int32)\n",
    "            item_ids = tf.cast(data[:, 1], dtype=tf.int32)\n",
    "            y_true = tf.cast(data[:, 2], dtype=tf.float32)\n",
    "\n",
    "            y_pred = self([user_ids, item_ids])\n",
    "            loss = self.compute_loss(y_true, y_pred)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def val_step(self, data):\n",
    "        user_ids = tf.cast(data[:, 0], dtype=tf.int32)\n",
    "        item_ids = tf.cast(data[:, 1], dtype=tf.int32)\n",
    "        y_true = tf.cast(data[:, 2], dtype=tf.float32)\n",
    "\n",
    "        y_pred = self([user_ids, item_ids])\n",
    "        loss = self.compute_loss(y_true, y_pred)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996</td>\n",
       "      <td>36978</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>836</td>\n",
       "      <td>6530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325</td>\n",
       "      <td>91669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214</td>\n",
       "      <td>125732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1168</td>\n",
       "      <td>144534</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>800</td>\n",
       "      <td>78793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>296</td>\n",
       "      <td>141731</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1817</td>\n",
       "      <td>25810</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1646</td>\n",
       "      <td>35006</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>213</td>\n",
       "      <td>101249</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  item_id  rating\n",
       "0       1996    36978       4\n",
       "1        836     6530       1\n",
       "2        325    91669       1\n",
       "3       1214   125732       1\n",
       "4       1168   144534       4\n",
       "..       ...      ...     ...\n",
       "995      800    78793       1\n",
       "996      296   141731       3\n",
       "997     1817    25810       4\n",
       "998     1646    35006       5\n",
       "999      213   101249       4\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = 2000\n",
    "num_items = 200000\n",
    "num_samples = 1000\n",
    "num_validation_samples = int(0.2 * num_samples)\n",
    "user_ids = np.random.randint(0, num_users, size=num_samples)\n",
    "item_ids = np.random.randint(0, num_items, size=num_samples)\n",
    "ratings = np.random.randint(1, 6, size=num_samples)\n",
    "\n",
    "data = {'user_id': user_ids, 'item_id': item_ids, 'rating': ratings}\n",
    "df_ratings = pd.DataFrame(data)\n",
    "df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996</td>\n",
       "      <td>36978</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>836</td>\n",
       "      <td>6530</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325</td>\n",
       "      <td>91669</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1214</td>\n",
       "      <td>125732</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1168</td>\n",
       "      <td>144534</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>540</td>\n",
       "      <td>164884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>1168</td>\n",
       "      <td>40589</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>333</td>\n",
       "      <td>101981</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>686</td>\n",
       "      <td>112988</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>506</td>\n",
       "      <td>64792</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  item_id  rating\n",
       "0       1996    36978     0.5\n",
       "1        836     6530    -1.0\n",
       "2        325    91669    -1.0\n",
       "3       1214   125732    -1.0\n",
       "4       1168   144534     0.5\n",
       "..       ...      ...     ...\n",
       "795      540   164884     0.0\n",
       "796     1168    40589     0.0\n",
       "797      333   101981    -0.5\n",
       "798      686   112988     0.5\n",
       "799      506    64792     1.0\n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_ratings.iloc[:800]\n",
    "df_train_norm = df_train.copy(deep=True)\n",
    "df_train_norm['rating'] -= 3\n",
    "df_train_norm['rating'] /= 2\n",
    "df_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>1320</td>\n",
       "      <td>66937</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>856</td>\n",
       "      <td>82820</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>1048</td>\n",
       "      <td>83107</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>1221</td>\n",
       "      <td>187504</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>1253</td>\n",
       "      <td>43407</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>800</td>\n",
       "      <td>78793</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>296</td>\n",
       "      <td>141731</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1817</td>\n",
       "      <td>25810</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1646</td>\n",
       "      <td>35006</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>213</td>\n",
       "      <td>101249</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  item_id  rating\n",
       "800     1320    66937     0.5\n",
       "801      856    82820    -1.0\n",
       "802     1048    83107     1.0\n",
       "803     1221   187504    -0.5\n",
       "804     1253    43407    -1.0\n",
       "..       ...      ...     ...\n",
       "995      800    78793    -1.0\n",
       "996      296   141731     0.0\n",
       "997     1817    25810     0.5\n",
       "998     1646    35006     1.0\n",
       "999      213   101249     0.5\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = df_ratings.iloc[800:]\n",
    "df_val_norm = df_val.copy(deep=True)\n",
    "df_val_norm['rating'] -= 3\n",
    "df_val_norm['rating'] /= 2\n",
    "df_val_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices(df_train_norm)\n",
    "dataset_train = dataset_train.batch(batch_size=BATCH_SIZE, num_parallel_calls=tf.data.AUTOTUNE, drop_remainder=True)\\\n",
    "                             .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices(df_val_norm)\n",
    "dataset_val = dataset_val.batch(batch_size=BATCH_SIZE, num_parallel_calls=tf.data.AUTOTUNE, drop_remainder=True)\\\n",
    "                         .prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"recommender_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  100000    \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     multiple                  10000000  \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " concatenate (Concatenate)   multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  12928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,113,057\n",
      "Trainable params: 10,113,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_users = 2000\n",
    "num_items = 200000\n",
    "model = RecommenderModel(num_users, num_items)\n",
    "model.build(input_shape=[(None, 1), (None, 1)])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 22:13:24.000121: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 0.7321, val_loss: 0.7113\n",
      "Epoch 2 train_loss: 0.7283, val_loss: 0.7085\n",
      "Epoch 3 train_loss: 0.7245, val_loss: 0.7057\n",
      "Epoch 4 train_loss: 0.7207, val_loss: 0.7029\n",
      "Epoch 5 train_loss: 0.7169, val_loss: 0.7000\n",
      "Epoch 6 train_loss: 0.7131, val_loss: 0.6972\n",
      "Epoch 7 train_loss: 0.7091, val_loss: 0.6943\n",
      "Epoch 8 train_loss: 0.7051, val_loss: 0.6913\n",
      "Epoch 9 train_loss: 0.7009, val_loss: 0.6883\n",
      "Epoch 10 train_loss: 0.6967, val_loss: 0.6853\n",
      "Epoch 11 train_loss: 0.6922, val_loss: 0.6822\n",
      "Epoch 12 train_loss: 0.6876, val_loss: 0.6790\n",
      "Epoch 13 train_loss: 0.6828, val_loss: 0.6757\n",
      "Epoch 14 train_loss: 0.6778, val_loss: 0.6724\n",
      "Epoch 15 train_loss: 0.6725, val_loss: 0.6690\n",
      "Epoch 16 train_loss: 0.6669, val_loss: 0.6655\n",
      "Epoch 17 train_loss: 0.6610, val_loss: 0.6620\n",
      "Epoch 18 train_loss: 0.6549, val_loss: 0.6583\n",
      "Epoch 19 train_loss: 0.6483, val_loss: 0.6546\n",
      "Epoch 20 train_loss: 0.6415, val_loss: 0.6509\n",
      "Epoch 21 train_loss: 0.6344, val_loss: 0.6471\n",
      "Epoch 22 train_loss: 0.6269, val_loss: 0.6432\n",
      "Epoch 23 train_loss: 0.6193, val_loss: 0.6394\n",
      "Epoch 24 train_loss: 0.6114, val_loss: 0.6355\n",
      "Epoch 25 train_loss: 0.6034, val_loss: 0.6316\n",
      "Epoch 26 train_loss: 0.5954, val_loss: 0.6278\n",
      "Epoch 27 train_loss: 0.5874, val_loss: 0.6240\n",
      "Epoch 28 train_loss: 0.5796, val_loss: 0.6204\n",
      "Epoch 29 train_loss: 0.5720, val_loss: 0.6168\n",
      "Epoch 30 train_loss: 0.5646, val_loss: 0.6133\n",
      "Epoch 31 train_loss: 0.5576, val_loss: 0.6100\n",
      "Epoch 32 train_loss: 0.5510, val_loss: 0.6068\n",
      "Epoch 33 train_loss: 0.5449, val_loss: 0.6037\n",
      "Epoch 34 train_loss: 0.5391, val_loss: 0.6009\n",
      "Epoch 35 train_loss: 0.5338, val_loss: 0.5981\n",
      "Epoch 36 train_loss: 0.5289, val_loss: 0.5956\n",
      "Epoch 37 train_loss: 0.5245, val_loss: 0.5932\n",
      "Epoch 38 train_loss: 0.5204, val_loss: 0.5909\n",
      "Epoch 39 train_loss: 0.5167, val_loss: 0.5888\n",
      "Epoch 40 train_loss: 0.5134, val_loss: 0.5868\n",
      "Epoch 41 train_loss: 0.5104, val_loss: 0.5849\n",
      "Epoch 42 train_loss: 0.5077, val_loss: 0.5832\n",
      "Epoch 43 train_loss: 0.5052, val_loss: 0.5816\n",
      "Epoch 44 train_loss: 0.5030, val_loss: 0.5801\n",
      "Epoch 45 train_loss: 0.5010, val_loss: 0.5787\n",
      "Epoch 46 train_loss: 0.4992, val_loss: 0.5773\n",
      "Epoch 47 train_loss: 0.4976, val_loss: 0.5761\n",
      "Epoch 48 train_loss: 0.4961, val_loss: 0.5749\n",
      "Epoch 49 train_loss: 0.4948, val_loss: 0.5738\n",
      "Epoch 50 train_loss: 0.4935, val_loss: 0.5728\n",
      "Epoch 51 train_loss: 0.4924, val_loss: 0.5718\n",
      "Epoch 52 train_loss: 0.4914, val_loss: 0.5709\n",
      "Epoch 53 train_loss: 0.4905, val_loss: 0.5700\n",
      "Epoch 54 train_loss: 0.4897, val_loss: 0.5692\n",
      "Epoch 55 train_loss: 0.4889, val_loss: 0.5684\n",
      "Epoch 56 train_loss: 0.4882, val_loss: 0.5677\n",
      "Epoch 57 train_loss: 0.4875, val_loss: 0.5670\n",
      "Epoch 58 train_loss: 0.4869, val_loss: 0.5663\n",
      "Epoch 59 train_loss: 0.4864, val_loss: 0.5657\n",
      "Epoch 60 train_loss: 0.4859, val_loss: 0.5651\n",
      "Epoch 61 train_loss: 0.4854, val_loss: 0.5645\n",
      "Epoch 62 train_loss: 0.4850, val_loss: 0.5640\n",
      "Epoch 63 train_loss: 0.4846, val_loss: 0.5635\n",
      "Epoch 64 train_loss: 0.4842, val_loss: 0.5630\n",
      "Epoch 65 train_loss: 0.4838, val_loss: 0.5625\n",
      "Epoch 66 train_loss: 0.4835, val_loss: 0.5621\n",
      "Epoch 67 train_loss: 0.4832, val_loss: 0.5616\n",
      "Epoch 68 train_loss: 0.4829, val_loss: 0.5612\n",
      "Epoch 69 train_loss: 0.4826, val_loss: 0.5608\n",
      "Epoch 70 train_loss: 0.4824, val_loss: 0.5604\n",
      "Epoch 71 train_loss: 0.4821, val_loss: 0.5601\n",
      "Epoch 72 train_loss: 0.4819, val_loss: 0.5597\n",
      "Epoch 73 train_loss: 0.4817, val_loss: 0.5594\n",
      "Epoch 74 train_loss: 0.4815, val_loss: 0.5591\n",
      "Epoch 75 train_loss: 0.4813, val_loss: 0.5587\n",
      "Epoch 76 train_loss: 0.4811, val_loss: 0.5584\n",
      "Epoch 77 train_loss: 0.4809, val_loss: 0.5582\n",
      "Epoch 78 train_loss: 0.4808, val_loss: 0.5579\n",
      "Epoch 79 train_loss: 0.4806, val_loss: 0.5576\n",
      "Epoch 80 train_loss: 0.4805, val_loss: 0.5573\n",
      "Epoch 81 train_loss: 0.4803, val_loss: 0.5571\n",
      "Epoch 82 train_loss: 0.4802, val_loss: 0.5569\n",
      "Epoch 83 train_loss: 0.4801, val_loss: 0.5566\n",
      "Epoch 84 train_loss: 0.4800, val_loss: 0.5564\n",
      "Epoch 85 train_loss: 0.4798, val_loss: 0.5562\n",
      "Epoch 86 train_loss: 0.4797, val_loss: 0.5560\n",
      "Epoch 87 train_loss: 0.4796, val_loss: 0.5557\n",
      "Epoch 88 train_loss: 0.4795, val_loss: 0.5555\n",
      "Epoch 89 train_loss: 0.4794, val_loss: 0.5554\n",
      "Epoch 90 train_loss: 0.4793, val_loss: 0.5552\n",
      "Epoch 91 train_loss: 0.4792, val_loss: 0.5550\n",
      "Epoch 92 train_loss: 0.4791, val_loss: 0.5548\n",
      "Epoch 93 train_loss: 0.4791, val_loss: 0.5546\n",
      "Epoch 94 train_loss: 0.4790, val_loss: 0.5545\n",
      "Epoch 95 train_loss: 0.4789, val_loss: 0.5543\n",
      "Epoch 96 train_loss: 0.4788, val_loss: 0.5541\n",
      "Epoch 97 train_loss: 0.4788, val_loss: 0.5540\n",
      "Epoch 98 train_loss: 0.4787, val_loss: 0.5538\n",
      "Epoch 99 train_loss: 0.4786, val_loss: 0.5537\n",
      "Epoch 100 train_loss: 0.4785, val_loss: 0.5535\n",
      "Epoch 101 train_loss: 0.4785, val_loss: 0.5534\n",
      "Epoch 102 train_loss: 0.4784, val_loss: 0.5533\n",
      "Epoch 103 train_loss: 0.4784, val_loss: 0.5531\n",
      "Epoch 104 train_loss: 0.4783, val_loss: 0.5530\n",
      "Epoch 105 train_loss: 0.4782, val_loss: 0.5529\n",
      "Epoch 106 train_loss: 0.4782, val_loss: 0.5528\n",
      "Epoch 107 train_loss: 0.4781, val_loss: 0.5526\n",
      "Epoch 108 train_loss: 0.4781, val_loss: 0.5525\n",
      "Epoch 109 train_loss: 0.4780, val_loss: 0.5524\n",
      "Epoch 110 train_loss: 0.4780, val_loss: 0.5523\n",
      "Epoch 111 train_loss: 0.4779, val_loss: 0.5522\n",
      "Epoch 112 train_loss: 0.4779, val_loss: 0.5521\n",
      "Epoch 113 train_loss: 0.4778, val_loss: 0.5520\n",
      "Epoch 114 train_loss: 0.4778, val_loss: 0.5519\n",
      "Epoch 115 train_loss: 0.4778, val_loss: 0.5518\n",
      "Epoch 116 train_loss: 0.4777, val_loss: 0.5517\n",
      "Epoch 117 train_loss: 0.4777, val_loss: 0.5516\n",
      "Epoch 118 train_loss: 0.4776, val_loss: 0.5515\n",
      "Epoch 119 train_loss: 0.4776, val_loss: 0.5514\n",
      "Epoch 120 train_loss: 0.4776, val_loss: 0.5513\n",
      "Epoch 121 train_loss: 0.4775, val_loss: 0.5512\n",
      "Epoch 122 train_loss: 0.4775, val_loss: 0.5511\n",
      "Epoch 123 train_loss: 0.4774, val_loss: 0.5510\n",
      "Epoch 124 train_loss: 0.4774, val_loss: 0.5509\n",
      "Epoch 125 train_loss: 0.4774, val_loss: 0.5509\n",
      "Epoch 126 train_loss: 0.4773, val_loss: 0.5508\n",
      "Epoch 127 train_loss: 0.4773, val_loss: 0.5507\n",
      "Epoch 128 train_loss: 0.4773, val_loss: 0.5506\n",
      "Epoch 129 train_loss: 0.4773, val_loss: 0.5505\n",
      "Epoch 130 train_loss: 0.4772, val_loss: 0.5505\n",
      "Epoch 131 train_loss: 0.4772, val_loss: 0.5504\n",
      "Epoch 132 train_loss: 0.4772, val_loss: 0.5503\n",
      "Epoch 133 train_loss: 0.4771, val_loss: 0.5502\n",
      "Epoch 134 train_loss: 0.4771, val_loss: 0.5501\n",
      "Epoch 135 train_loss: 0.4771, val_loss: 0.5501\n",
      "Epoch 136 train_loss: 0.4771, val_loss: 0.5500\n",
      "Epoch 137 train_loss: 0.4770, val_loss: 0.5499\n",
      "Epoch 138 train_loss: 0.4770, val_loss: 0.5499\n",
      "Epoch 139 train_loss: 0.4770, val_loss: 0.5498\n",
      "Epoch 140 train_loss: 0.4770, val_loss: 0.5497\n",
      "Epoch 141 train_loss: 0.4769, val_loss: 0.5496\n",
      "Epoch 142 train_loss: 0.4769, val_loss: 0.5496\n",
      "Epoch 143 train_loss: 0.4769, val_loss: 0.5495\n",
      "Epoch 144 train_loss: 0.4769, val_loss: 0.5494\n",
      "Epoch 145 train_loss: 0.4768, val_loss: 0.5494\n",
      "Epoch 146 train_loss: 0.4768, val_loss: 0.5493\n",
      "Epoch 147 train_loss: 0.4768, val_loss: 0.5492\n",
      "Epoch 148 train_loss: 0.4768, val_loss: 0.5492\n",
      "Epoch 149 train_loss: 0.4768, val_loss: 0.5491\n",
      "Epoch 150 train_loss: 0.4767, val_loss: 0.5490\n",
      "Epoch 151 train_loss: 0.4767, val_loss: 0.5490\n",
      "Epoch 152 train_loss: 0.4767, val_loss: 0.5489\n",
      "Epoch 153 train_loss: 0.4767, val_loss: 0.5489\n",
      "Epoch 154 train_loss: 0.4767, val_loss: 0.5488\n",
      "Epoch 155 train_loss: 0.4767, val_loss: 0.5487\n",
      "Epoch 156 train_loss: 0.4766, val_loss: 0.5487\n",
      "Epoch 157 train_loss: 0.4766, val_loss: 0.5486\n",
      "Epoch 158 train_loss: 0.4766, val_loss: 0.5485\n",
      "Epoch 159 train_loss: 0.4766, val_loss: 0.5485\n",
      "Epoch 160 train_loss: 0.4766, val_loss: 0.5484\n",
      "Epoch 161 train_loss: 0.4766, val_loss: 0.5484\n",
      "Epoch 162 train_loss: 0.4765, val_loss: 0.5483\n",
      "Epoch 163 train_loss: 0.4765, val_loss: 0.5483\n",
      "Epoch 164 train_loss: 0.4765, val_loss: 0.5482\n",
      "Epoch 165 train_loss: 0.4765, val_loss: 0.5481\n",
      "Epoch 166 train_loss: 0.4765, val_loss: 0.5481\n",
      "Epoch 167 train_loss: 0.4765, val_loss: 0.5480\n",
      "Epoch 168 train_loss: 0.4765, val_loss: 0.5480\n",
      "Epoch 169 train_loss: 0.4764, val_loss: 0.5479\n",
      "Epoch 170 train_loss: 0.4764, val_loss: 0.5479\n",
      "Epoch 171 train_loss: 0.4764, val_loss: 0.5478\n",
      "Epoch 172 train_loss: 0.4764, val_loss: 0.5477\n",
      "Epoch 173 train_loss: 0.4764, val_loss: 0.5477\n",
      "Epoch 174 train_loss: 0.4764, val_loss: 0.5476\n",
      "Epoch 175 train_loss: 0.4764, val_loss: 0.5476\n",
      "Epoch 176 train_loss: 0.4764, val_loss: 0.5475\n",
      "Epoch 177 train_loss: 0.4763, val_loss: 0.5475\n",
      "Epoch 178 train_loss: 0.4763, val_loss: 0.5474\n",
      "Epoch 179 train_loss: 0.4763, val_loss: 0.5474\n",
      "Epoch 180 train_loss: 0.4763, val_loss: 0.5473\n",
      "Epoch 181 train_loss: 0.4763, val_loss: 0.5473\n",
      "Epoch 182 train_loss: 0.4763, val_loss: 0.5472\n",
      "Epoch 183 train_loss: 0.4763, val_loss: 0.5472\n",
      "Epoch 184 train_loss: 0.4763, val_loss: 0.5471\n",
      "Epoch 185 train_loss: 0.4763, val_loss: 0.5471\n",
      "Epoch 186 train_loss: 0.4763, val_loss: 0.5470\n",
      "Epoch 187 train_loss: 0.4762, val_loss: 0.5470\n",
      "Epoch 188 train_loss: 0.4762, val_loss: 0.5469\n",
      "Epoch 189 train_loss: 0.4762, val_loss: 0.5469\n",
      "Epoch 190 train_loss: 0.4762, val_loss: 0.5468\n",
      "Epoch 191 train_loss: 0.4762, val_loss: 0.5468\n",
      "Epoch 192 train_loss: 0.4762, val_loss: 0.5467\n",
      "Epoch 193 train_loss: 0.4762, val_loss: 0.5467\n",
      "Epoch 194 train_loss: 0.4762, val_loss: 0.5466\n",
      "Epoch 195 train_loss: 0.4762, val_loss: 0.5466\n",
      "Epoch 196 train_loss: 0.4762, val_loss: 0.5465\n",
      "Epoch 197 train_loss: 0.4762, val_loss: 0.5465\n",
      "Epoch 198 train_loss: 0.4762, val_loss: 0.5464\n",
      "Epoch 199 train_loss: 0.4762, val_loss: 0.5464\n",
      "Epoch 200 train_loss: 0.4761, val_loss: 0.5463\n",
      "Epoch 201 train_loss: 0.4761, val_loss: 0.5463\n",
      "Epoch 202 train_loss: 0.4761, val_loss: 0.5463\n",
      "Epoch 203 train_loss: 0.4761, val_loss: 0.5462\n",
      "Epoch 204 train_loss: 0.4761, val_loss: 0.5462\n",
      "Epoch 205 train_loss: 0.4761, val_loss: 0.5461\n",
      "Epoch 206 train_loss: 0.4761, val_loss: 0.5461\n",
      "Epoch 207 train_loss: 0.4761, val_loss: 0.5460\n",
      "Epoch 208 train_loss: 0.4761, val_loss: 0.5460\n",
      "Epoch 209 train_loss: 0.4761, val_loss: 0.5460\n",
      "Epoch 210 train_loss: 0.4761, val_loss: 0.5459\n",
      "Epoch 211 train_loss: 0.4761, val_loss: 0.5459\n",
      "Epoch 212 train_loss: 0.4761, val_loss: 0.5458\n",
      "Epoch 213 train_loss: 0.4761, val_loss: 0.5458\n",
      "Epoch 214 train_loss: 0.4761, val_loss: 0.5458\n",
      "Epoch 215 train_loss: 0.4760, val_loss: 0.5457\n",
      "Epoch 216 train_loss: 0.4760, val_loss: 0.5457\n",
      "Epoch 217 train_loss: 0.4760, val_loss: 0.5456\n",
      "Epoch 218 train_loss: 0.4760, val_loss: 0.5456\n",
      "Epoch 219 train_loss: 0.4760, val_loss: 0.5456\n",
      "Epoch 220 train_loss: 0.4760, val_loss: 0.5455\n",
      "Epoch 221 train_loss: 0.4760, val_loss: 0.5455\n",
      "Epoch 222 train_loss: 0.4760, val_loss: 0.5454\n",
      "Epoch 223 train_loss: 0.4760, val_loss: 0.5454\n",
      "Epoch 224 train_loss: 0.4760, val_loss: 0.5454\n",
      "Epoch 225 train_loss: 0.4760, val_loss: 0.5453\n",
      "Epoch 226 train_loss: 0.4760, val_loss: 0.5453\n",
      "Epoch 227 train_loss: 0.4760, val_loss: 0.5452\n",
      "Epoch 228 train_loss: 0.4760, val_loss: 0.5452\n",
      "Epoch 229 train_loss: 0.4760, val_loss: 0.5452\n",
      "Epoch 230 train_loss: 0.4760, val_loss: 0.5451\n",
      "Epoch 231 train_loss: 0.4760, val_loss: 0.5451\n",
      "Epoch 232 train_loss: 0.4760, val_loss: 0.5451\n",
      "Epoch 233 train_loss: 0.4760, val_loss: 0.5450\n",
      "Epoch 234 train_loss: 0.4760, val_loss: 0.5450\n",
      "Epoch 235 train_loss: 0.4760, val_loss: 0.5450\n",
      "Epoch 236 train_loss: 0.4759, val_loss: 0.5449\n",
      "Epoch 237 train_loss: 0.4759, val_loss: 0.5449\n",
      "Epoch 238 train_loss: 0.4759, val_loss: 0.5449\n",
      "Epoch 239 train_loss: 0.4759, val_loss: 0.5448\n",
      "Epoch 240 train_loss: 0.4759, val_loss: 0.5448\n",
      "Epoch 241 train_loss: 0.4759, val_loss: 0.5448\n",
      "Epoch 242 train_loss: 0.4759, val_loss: 0.5447\n",
      "Epoch 243 train_loss: 0.4759, val_loss: 0.5447\n",
      "Epoch 244 train_loss: 0.4759, val_loss: 0.5447\n",
      "Epoch 245 train_loss: 0.4759, val_loss: 0.5446\n",
      "Epoch 246 train_loss: 0.4759, val_loss: 0.5446\n",
      "Epoch 247 train_loss: 0.4759, val_loss: 0.5446\n",
      "Epoch 248 train_loss: 0.4759, val_loss: 0.5445\n",
      "Epoch 249 train_loss: 0.4759, val_loss: 0.5445\n",
      "Epoch 250 train_loss: 0.4759, val_loss: 0.5445\n",
      "Epoch 251 train_loss: 0.4759, val_loss: 0.5444\n",
      "Epoch 252 train_loss: 0.4759, val_loss: 0.5444\n",
      "Epoch 253 train_loss: 0.4759, val_loss: 0.5444\n",
      "Epoch 254 train_loss: 0.4759, val_loss: 0.5443\n",
      "Epoch 255 train_loss: 0.4759, val_loss: 0.5443\n",
      "Epoch 256 train_loss: 0.4759, val_loss: 0.5443\n",
      "Epoch 257 train_loss: 0.4759, val_loss: 0.5443\n",
      "Epoch 258 train_loss: 0.4759, val_loss: 0.5442\n",
      "Epoch 259 train_loss: 0.4759, val_loss: 0.5442\n",
      "Epoch 260 train_loss: 0.4759, val_loss: 0.5442\n",
      "Epoch 261 train_loss: 0.4759, val_loss: 0.5441\n",
      "Epoch 262 train_loss: 0.4759, val_loss: 0.5441\n",
      "Epoch 263 train_loss: 0.4759, val_loss: 0.5441\n",
      "Epoch 264 train_loss: 0.4759, val_loss: 0.5440\n",
      "Epoch 265 train_loss: 0.4758, val_loss: 0.5440\n",
      "Epoch 266 train_loss: 0.4758, val_loss: 0.5440\n",
      "Epoch 267 train_loss: 0.4758, val_loss: 0.5440\n",
      "Epoch 268 train_loss: 0.4758, val_loss: 0.5439\n",
      "Epoch 269 train_loss: 0.4758, val_loss: 0.5439\n",
      "Epoch 270 train_loss: 0.4758, val_loss: 0.5439\n",
      "Epoch 271 train_loss: 0.4758, val_loss: 0.5439\n",
      "Epoch 272 train_loss: 0.4758, val_loss: 0.5438\n",
      "Epoch 273 train_loss: 0.4758, val_loss: 0.5438\n",
      "Epoch 274 train_loss: 0.4758, val_loss: 0.5438\n",
      "Epoch 275 train_loss: 0.4758, val_loss: 0.5437\n",
      "Epoch 276 train_loss: 0.4758, val_loss: 0.5437\n",
      "Epoch 277 train_loss: 0.4758, val_loss: 0.5437\n",
      "Epoch 278 train_loss: 0.4758, val_loss: 0.5437\n",
      "Epoch 279 train_loss: 0.4758, val_loss: 0.5436\n",
      "Epoch 280 train_loss: 0.4758, val_loss: 0.5436\n",
      "Epoch 281 train_loss: 0.4758, val_loss: 0.5436\n",
      "Epoch 282 train_loss: 0.4758, val_loss: 0.5436\n",
      "Epoch 283 train_loss: 0.4758, val_loss: 0.5435\n",
      "Epoch 284 train_loss: 0.4758, val_loss: 0.5435\n",
      "Epoch 285 train_loss: 0.4758, val_loss: 0.5435\n",
      "Epoch 286 train_loss: 0.4758, val_loss: 0.5435\n",
      "Epoch 287 train_loss: 0.4758, val_loss: 0.5434\n",
      "Epoch 288 train_loss: 0.4758, val_loss: 0.5434\n",
      "Epoch 289 train_loss: 0.4758, val_loss: 0.5434\n",
      "Epoch 290 train_loss: 0.4758, val_loss: 0.5434\n",
      "Epoch 291 train_loss: 0.4758, val_loss: 0.5433\n",
      "Epoch 292 train_loss: 0.4758, val_loss: 0.5433\n",
      "Epoch 293 train_loss: 0.4758, val_loss: 0.5433\n",
      "Epoch 294 train_loss: 0.4758, val_loss: 0.5433\n",
      "Epoch 295 train_loss: 0.4758, val_loss: 0.5432\n",
      "Epoch 296 train_loss: 0.4758, val_loss: 0.5432\n",
      "Epoch 297 train_loss: 0.4758, val_loss: 0.5432\n",
      "Epoch 298 train_loss: 0.4758, val_loss: 0.5432\n",
      "Epoch 299 train_loss: 0.4758, val_loss: 0.5431\n",
      "Epoch 300 train_loss: 0.4758, val_loss: 0.5431\n",
      "Epoch 301 train_loss: 0.4758, val_loss: 0.5431\n",
      "Epoch 302 train_loss: 0.4758, val_loss: 0.5431\n",
      "Epoch 303 train_loss: 0.4758, val_loss: 0.5430\n",
      "Epoch 304 train_loss: 0.4758, val_loss: 0.5430\n",
      "Epoch 305 train_loss: 0.4758, val_loss: 0.5430\n",
      "Epoch 306 train_loss: 0.4758, val_loss: 0.5430\n",
      "Epoch 307 train_loss: 0.4758, val_loss: 0.5430\n",
      "Epoch 308 train_loss: 0.4758, val_loss: 0.5429\n",
      "Epoch 309 train_loss: 0.4757, val_loss: 0.5429\n",
      "Epoch 310 train_loss: 0.4757, val_loss: 0.5429\n",
      "Epoch 311 train_loss: 0.4757, val_loss: 0.5429\n",
      "Epoch 312 train_loss: 0.4757, val_loss: 0.5428\n",
      "Epoch 313 train_loss: 0.4757, val_loss: 0.5428\n",
      "Epoch 314 train_loss: 0.4757, val_loss: 0.5428\n",
      "Epoch 315 train_loss: 0.4757, val_loss: 0.5428\n",
      "Epoch 316 train_loss: 0.4757, val_loss: 0.5427\n",
      "Epoch 317 train_loss: 0.4757, val_loss: 0.5427\n",
      "Epoch 318 train_loss: 0.4757, val_loss: 0.5427\n",
      "Epoch 319 train_loss: 0.4757, val_loss: 0.5427\n",
      "Epoch 320 train_loss: 0.4757, val_loss: 0.5427\n",
      "Epoch 321 train_loss: 0.4757, val_loss: 0.5426\n",
      "Epoch 322 train_loss: 0.4757, val_loss: 0.5426\n",
      "Epoch 323 train_loss: 0.4757, val_loss: 0.5426\n",
      "Epoch 324 train_loss: 0.4757, val_loss: 0.5426\n",
      "Epoch 325 train_loss: 0.4757, val_loss: 0.5426\n",
      "Epoch 326 train_loss: 0.4757, val_loss: 0.5425\n",
      "Epoch 327 train_loss: 0.4757, val_loss: 0.5425\n",
      "Epoch 328 train_loss: 0.4757, val_loss: 0.5425\n",
      "Epoch 329 train_loss: 0.4757, val_loss: 0.5425\n",
      "Epoch 330 train_loss: 0.4757, val_loss: 0.5424\n",
      "Epoch 331 train_loss: 0.4757, val_loss: 0.5424\n",
      "Epoch 332 train_loss: 0.4757, val_loss: 0.5424\n",
      "Epoch 333 train_loss: 0.4757, val_loss: 0.5424\n",
      "Epoch 334 train_loss: 0.4757, val_loss: 0.5424\n",
      "Epoch 335 train_loss: 0.4757, val_loss: 0.5423\n",
      "Epoch 336 train_loss: 0.4757, val_loss: 0.5423\n",
      "Epoch 337 train_loss: 0.4757, val_loss: 0.5423\n",
      "Epoch 338 train_loss: 0.4757, val_loss: 0.5423\n",
      "Epoch 339 train_loss: 0.4757, val_loss: 0.5423\n",
      "Epoch 340 train_loss: 0.4757, val_loss: 0.5422\n",
      "Epoch 341 train_loss: 0.4757, val_loss: 0.5422\n",
      "Epoch 342 train_loss: 0.4757, val_loss: 0.5422\n",
      "Epoch 343 train_loss: 0.4757, val_loss: 0.5422\n",
      "Epoch 344 train_loss: 0.4757, val_loss: 0.5421\n",
      "Epoch 345 train_loss: 0.4757, val_loss: 0.5421\n",
      "Epoch 346 train_loss: 0.4757, val_loss: 0.5421\n",
      "Epoch 347 train_loss: 0.4757, val_loss: 0.5421\n",
      "Epoch 348 train_loss: 0.4757, val_loss: 0.5421\n",
      "Epoch 349 train_loss: 0.4757, val_loss: 0.5420\n",
      "Epoch 350 train_loss: 0.4757, val_loss: 0.5420\n",
      "Epoch 351 train_loss: 0.4757, val_loss: 0.5420\n",
      "Epoch 352 train_loss: 0.4757, val_loss: 0.5420\n",
      "Epoch 353 train_loss: 0.4757, val_loss: 0.5420\n",
      "Epoch 354 train_loss: 0.4757, val_loss: 0.5419\n",
      "Epoch 355 train_loss: 0.4757, val_loss: 0.5419\n",
      "Epoch 356 train_loss: 0.4757, val_loss: 0.5419\n",
      "Epoch 357 train_loss: 0.4757, val_loss: 0.5419\n",
      "Epoch 358 train_loss: 0.4757, val_loss: 0.5419\n",
      "Epoch 359 train_loss: 0.4757, val_loss: 0.5418\n",
      "Epoch 360 train_loss: 0.4757, val_loss: 0.5418\n",
      "Epoch 361 train_loss: 0.4757, val_loss: 0.5418\n",
      "Epoch 362 train_loss: 0.4757, val_loss: 0.5418\n",
      "Epoch 363 train_loss: 0.4757, val_loss: 0.5418\n",
      "Epoch 364 train_loss: 0.4757, val_loss: 0.5417\n",
      "Epoch 365 train_loss: 0.4757, val_loss: 0.5417\n",
      "Epoch 366 train_loss: 0.4757, val_loss: 0.5417\n",
      "Epoch 367 train_loss: 0.4757, val_loss: 0.5417\n",
      "Epoch 368 train_loss: 0.4757, val_loss: 0.5417\n",
      "Epoch 369 train_loss: 0.4757, val_loss: 0.5416\n",
      "Epoch 370 train_loss: 0.4757, val_loss: 0.5416\n",
      "Epoch 371 train_loss: 0.4757, val_loss: 0.5416\n",
      "Epoch 372 train_loss: 0.4757, val_loss: 0.5416\n",
      "Epoch 373 train_loss: 0.4757, val_loss: 0.5416\n",
      "Epoch 374 train_loss: 0.4757, val_loss: 0.5415\n",
      "Epoch 375 train_loss: 0.4757, val_loss: 0.5415\n",
      "Epoch 376 train_loss: 0.4757, val_loss: 0.5415\n",
      "Epoch 377 train_loss: 0.4757, val_loss: 0.5415\n",
      "Epoch 378 train_loss: 0.4757, val_loss: 0.5415\n",
      "Epoch 379 train_loss: 0.4757, val_loss: 0.5414\n",
      "Epoch 380 train_loss: 0.4757, val_loss: 0.5414\n",
      "Epoch 381 train_loss: 0.4757, val_loss: 0.5414\n",
      "Epoch 382 train_loss: 0.4757, val_loss: 0.5414\n",
      "Epoch 383 train_loss: 0.4757, val_loss: 0.5414\n",
      "Epoch 384 train_loss: 0.4757, val_loss: 0.5414\n",
      "Epoch 385 train_loss: 0.4757, val_loss: 0.5413\n",
      "Epoch 386 train_loss: 0.4757, val_loss: 0.5413\n",
      "Epoch 387 train_loss: 0.4757, val_loss: 0.5413\n",
      "Epoch 388 train_loss: 0.4757, val_loss: 0.5413\n",
      "Epoch 389 train_loss: 0.4757, val_loss: 0.5413\n",
      "Epoch 390 train_loss: 0.4757, val_loss: 0.5412\n",
      "Epoch 391 train_loss: 0.4757, val_loss: 0.5412\n",
      "Epoch 392 train_loss: 0.4756, val_loss: 0.5412\n",
      "Epoch 393 train_loss: 0.4756, val_loss: 0.5412\n",
      "Epoch 394 train_loss: 0.4756, val_loss: 0.5412\n",
      "Epoch 395 train_loss: 0.4756, val_loss: 0.5411\n",
      "Epoch 396 train_loss: 0.4756, val_loss: 0.5411\n",
      "Epoch 397 train_loss: 0.4756, val_loss: 0.5411\n",
      "Epoch 398 train_loss: 0.4756, val_loss: 0.5411\n",
      "Epoch 399 train_loss: 0.4756, val_loss: 0.5411\n",
      "Epoch 400 train_loss: 0.4756, val_loss: 0.5411\n",
      "Epoch 401 train_loss: 0.4756, val_loss: 0.5410\n",
      "Epoch 402 train_loss: 0.4756, val_loss: 0.5410\n",
      "Epoch 403 train_loss: 0.4756, val_loss: 0.5410\n",
      "Epoch 404 train_loss: 0.4756, val_loss: 0.5410\n",
      "Epoch 405 train_loss: 0.4756, val_loss: 0.5410\n",
      "Epoch 406 train_loss: 0.4756, val_loss: 0.5409\n",
      "Epoch 407 train_loss: 0.4756, val_loss: 0.5409\n",
      "Epoch 408 train_loss: 0.4756, val_loss: 0.5409\n",
      "Epoch 409 train_loss: 0.4756, val_loss: 0.5409\n",
      "Epoch 410 train_loss: 0.4756, val_loss: 0.5409\n",
      "Epoch 411 train_loss: 0.4756, val_loss: 0.5409\n",
      "Epoch 412 train_loss: 0.4756, val_loss: 0.5408\n",
      "Epoch 413 train_loss: 0.4756, val_loss: 0.5408\n",
      "Epoch 414 train_loss: 0.4756, val_loss: 0.5408\n",
      "Epoch 415 train_loss: 0.4756, val_loss: 0.5408\n",
      "Epoch 416 train_loss: 0.4756, val_loss: 0.5408\n",
      "Epoch 417 train_loss: 0.4756, val_loss: 0.5408\n",
      "Epoch 418 train_loss: 0.4756, val_loss: 0.5407\n",
      "Epoch 419 train_loss: 0.4756, val_loss: 0.5407\n",
      "Epoch 420 train_loss: 0.4756, val_loss: 0.5407\n",
      "Epoch 421 train_loss: 0.4756, val_loss: 0.5407\n",
      "Epoch 422 train_loss: 0.4756, val_loss: 0.5407\n",
      "Epoch 423 train_loss: 0.4756, val_loss: 0.5407\n",
      "Epoch 424 train_loss: 0.4756, val_loss: 0.5406\n",
      "Epoch 425 train_loss: 0.4756, val_loss: 0.5406\n",
      "Epoch 426 train_loss: 0.4756, val_loss: 0.5406\n",
      "Epoch 427 train_loss: 0.4756, val_loss: 0.5406\n",
      "Epoch 428 train_loss: 0.4756, val_loss: 0.5406\n",
      "Epoch 429 train_loss: 0.4756, val_loss: 0.5406\n",
      "Epoch 430 train_loss: 0.4756, val_loss: 0.5405\n",
      "Epoch 431 train_loss: 0.4756, val_loss: 0.5405\n",
      "Epoch 432 train_loss: 0.4756, val_loss: 0.5405\n",
      "Epoch 433 train_loss: 0.4756, val_loss: 0.5405\n",
      "Epoch 434 train_loss: 0.4756, val_loss: 0.5405\n",
      "Epoch 435 train_loss: 0.4756, val_loss: 0.5405\n",
      "Epoch 436 train_loss: 0.4756, val_loss: 0.5405\n",
      "Epoch 437 train_loss: 0.4756, val_loss: 0.5404\n",
      "Epoch 438 train_loss: 0.4756, val_loss: 0.5404\n",
      "Epoch 439 train_loss: 0.4756, val_loss: 0.5404\n",
      "Epoch 440 train_loss: 0.4756, val_loss: 0.5404\n",
      "Epoch 441 train_loss: 0.4756, val_loss: 0.5404\n",
      "Epoch 442 train_loss: 0.4756, val_loss: 0.5404\n",
      "Epoch 443 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 444 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 445 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 446 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 447 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 448 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 449 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 450 train_loss: 0.4756, val_loss: 0.5403\n",
      "Epoch 451 train_loss: 0.4756, val_loss: 0.5402\n",
      "Epoch 452 train_loss: 0.4756, val_loss: 0.5402\n",
      "Epoch 453 train_loss: 0.4756, val_loss: 0.5402\n",
      "Epoch 454 train_loss: 0.4756, val_loss: 0.5402\n",
      "Epoch 455 train_loss: 0.4756, val_loss: 0.5402\n",
      "Epoch 456 train_loss: 0.4756, val_loss: 0.5402\n",
      "Epoch 457 train_loss: 0.4756, val_loss: 0.5402\n",
      "Epoch 458 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 459 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 460 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 461 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 462 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 463 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 464 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 465 train_loss: 0.4756, val_loss: 0.5401\n",
      "Epoch 466 train_loss: 0.4756, val_loss: 0.5400\n",
      "Epoch 467 train_loss: 0.4756, val_loss: 0.5400\n",
      "Epoch 468 train_loss: 0.4756, val_loss: 0.5400\n",
      "Epoch 469 train_loss: 0.4756, val_loss: 0.5400\n",
      "Epoch 470 train_loss: 0.4756, val_loss: 0.5400\n",
      "Epoch 471 train_loss: 0.4756, val_loss: 0.5400\n",
      "Epoch 472 train_loss: 0.4756, val_loss: 0.5400\n",
      "Epoch 473 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 474 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 475 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 476 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 477 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 478 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 479 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 480 train_loss: 0.4756, val_loss: 0.5399\n",
      "Epoch 481 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 482 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 483 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 484 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 485 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 486 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 487 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 488 train_loss: 0.4756, val_loss: 0.5398\n",
      "Epoch 489 train_loss: 0.4756, val_loss: 0.5397\n",
      "Epoch 490 train_loss: 0.4756, val_loss: 0.5397\n",
      "Epoch 491 train_loss: 0.4756, val_loss: 0.5397\n",
      "Epoch 492 train_loss: 0.4756, val_loss: 0.5397\n",
      "Epoch 493 train_loss: 0.4756, val_loss: 0.5397\n",
      "Epoch 494 train_loss: 0.4756, val_loss: 0.5397\n",
      "Epoch 495 train_loss: 0.4756, val_loss: 0.5397\n",
      "Epoch 496 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 497 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 498 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 499 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 500 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 501 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 502 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 503 train_loss: 0.4756, val_loss: 0.5396\n",
      "Epoch 504 train_loss: 0.4756, val_loss: 0.5395\n",
      "Epoch 505 train_loss: 0.4756, val_loss: 0.5395\n",
      "Epoch 506 train_loss: 0.4756, val_loss: 0.5395\n",
      "Epoch 507 train_loss: 0.4756, val_loss: 0.5395\n",
      "Epoch 508 train_loss: 0.4756, val_loss: 0.5395\n",
      "Epoch 509 train_loss: 0.4756, val_loss: 0.5395\n",
      "Epoch 510 train_loss: 0.4756, val_loss: 0.5395\n",
      "Epoch 511 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 512 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 513 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 514 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 515 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 516 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 517 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 518 train_loss: 0.4756, val_loss: 0.5394\n",
      "Epoch 519 train_loss: 0.4756, val_loss: 0.5393\n",
      "Epoch 520 train_loss: 0.4756, val_loss: 0.5393\n",
      "Epoch 521 train_loss: 0.4756, val_loss: 0.5393\n",
      "Epoch 522 train_loss: 0.4756, val_loss: 0.5393\n",
      "Epoch 523 train_loss: 0.4756, val_loss: 0.5393\n",
      "Epoch 524 train_loss: 0.4756, val_loss: 0.5393\n",
      "Epoch 525 train_loss: 0.4756, val_loss: 0.5393\n",
      "Epoch 526 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 527 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 528 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 529 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 530 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 531 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 532 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 533 train_loss: 0.4756, val_loss: 0.5392\n",
      "Epoch 534 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 535 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 536 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 537 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 538 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 539 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 540 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 541 train_loss: 0.4756, val_loss: 0.5391\n",
      "Epoch 542 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 543 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 544 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 545 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 546 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 547 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 548 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 549 train_loss: 0.4756, val_loss: 0.5390\n",
      "Epoch 550 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 551 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 552 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 553 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 554 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 555 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 556 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 557 train_loss: 0.4756, val_loss: 0.5389\n",
      "Epoch 558 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 559 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 560 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 561 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 562 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 563 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 564 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 565 train_loss: 0.4756, val_loss: 0.5388\n",
      "Epoch 566 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 567 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 568 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 569 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 570 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 571 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 572 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 573 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 574 train_loss: 0.4756, val_loss: 0.5387\n",
      "Epoch 575 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 576 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 577 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 578 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 579 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 580 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 581 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 582 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 583 train_loss: 0.4756, val_loss: 0.5386\n",
      "Epoch 584 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 585 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 586 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 587 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 588 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 589 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 590 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 591 train_loss: 0.4756, val_loss: 0.5385\n",
      "Epoch 592 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 593 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 594 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 595 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 596 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 597 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 598 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 599 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 600 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 601 train_loss: 0.4756, val_loss: 0.5384\n",
      "Epoch 602 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 603 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 604 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 605 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 606 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 607 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 608 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 609 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 610 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 611 train_loss: 0.4756, val_loss: 0.5383\n",
      "Epoch 612 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 613 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 614 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 615 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 616 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 617 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 618 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 619 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 620 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 621 train_loss: 0.4756, val_loss: 0.5382\n",
      "Epoch 622 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 623 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 624 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 625 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 626 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 627 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 628 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 629 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 630 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 631 train_loss: 0.4756, val_loss: 0.5381\n",
      "Epoch 632 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 633 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 634 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 635 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 636 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 637 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 638 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 639 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 640 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 641 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 642 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 643 train_loss: 0.4756, val_loss: 0.5380\n",
      "Epoch 644 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 645 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 646 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 647 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 648 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 649 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 650 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 651 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 652 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 653 train_loss: 0.4756, val_loss: 0.5379\n",
      "Epoch 654 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 655 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 656 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 657 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 658 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 659 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 660 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 661 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 662 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 663 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 664 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 665 train_loss: 0.4756, val_loss: 0.5378\n",
      "Epoch 666 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 667 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 668 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 669 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 670 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 671 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 672 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 673 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 674 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 675 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 676 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 677 train_loss: 0.4756, val_loss: 0.5377\n",
      "Epoch 678 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 679 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 680 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 681 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 682 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 683 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 684 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 685 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 686 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 687 train_loss: 0.4756, val_loss: 0.5376\n",
      "Epoch 688 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 689 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 690 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 691 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 692 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 693 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 694 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 695 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 696 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 697 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 698 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 699 train_loss: 0.4756, val_loss: 0.5375\n",
      "Epoch 700 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 701 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 702 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 703 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 704 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 705 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 706 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 707 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 708 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 709 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 710 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 711 train_loss: 0.4756, val_loss: 0.5374\n",
      "Epoch 712 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 713 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 714 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 715 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 716 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 717 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 718 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 719 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 720 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 721 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 722 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 723 train_loss: 0.4756, val_loss: 0.5373\n",
      "Epoch 724 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 725 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 726 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 727 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 728 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 729 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 730 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 731 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 732 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 733 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 734 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 735 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 736 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 737 train_loss: 0.4756, val_loss: 0.5372\n",
      "Epoch 738 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 739 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 740 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 741 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 742 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 743 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 744 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 745 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 746 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 747 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 748 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 749 train_loss: 0.4756, val_loss: 0.5371\n",
      "Epoch 750 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 751 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 752 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 753 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 754 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 755 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 756 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 757 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 758 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 759 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 760 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 761 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 762 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 763 train_loss: 0.4756, val_loss: 0.5370\n",
      "Epoch 764 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 765 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 766 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 767 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 768 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 769 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 770 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 771 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 772 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 773 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 774 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 775 train_loss: 0.4756, val_loss: 0.5369\n",
      "Epoch 776 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 777 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 778 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 779 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 780 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 781 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 782 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 783 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 784 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 785 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 786 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 787 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 788 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 789 train_loss: 0.4756, val_loss: 0.5368\n",
      "Epoch 790 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 791 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 792 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 793 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 794 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 795 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 796 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 797 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 798 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 799 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 800 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 801 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 802 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 803 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 804 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 805 train_loss: 0.4756, val_loss: 0.5367\n",
      "Epoch 806 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 807 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 808 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 809 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 810 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 811 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 812 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 813 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 814 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 815 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 816 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 817 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 818 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 819 train_loss: 0.4756, val_loss: 0.5366\n",
      "Epoch 820 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 821 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 822 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 823 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 824 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 825 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 826 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 827 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 828 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 829 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 830 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 831 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 832 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 833 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 834 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 835 train_loss: 0.4756, val_loss: 0.5365\n",
      "Epoch 836 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 837 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 838 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 839 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 840 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 841 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 842 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 843 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 844 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 845 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 846 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 847 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 848 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 849 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 850 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 851 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 852 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 853 train_loss: 0.4756, val_loss: 0.5364\n",
      "Epoch 854 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 855 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 856 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 857 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 858 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 859 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 860 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 861 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 862 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 863 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 864 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 865 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 866 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 867 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 868 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 869 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 870 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 871 train_loss: 0.4756, val_loss: 0.5363\n",
      "Epoch 872 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 873 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 874 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 875 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 876 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 877 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 878 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 879 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 880 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 881 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 882 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 883 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 884 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 885 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 886 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 887 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 888 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 889 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 890 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 891 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 892 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 893 train_loss: 0.4756, val_loss: 0.5362\n",
      "Epoch 894 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 895 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 896 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 897 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 898 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 899 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 900 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 901 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 902 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 903 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 904 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 905 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 906 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 907 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 908 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 909 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 910 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 911 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 912 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 913 train_loss: 0.4756, val_loss: 0.5361\n",
      "Epoch 914 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 915 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 916 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 917 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 918 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 919 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 920 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 921 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 922 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 923 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 924 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 925 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 926 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 927 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 928 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 929 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 930 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 931 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 932 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 933 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 934 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 935 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 936 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 937 train_loss: 0.4756, val_loss: 0.5360\n",
      "Epoch 938 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 939 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 940 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 941 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 942 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 943 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 944 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 945 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 946 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 947 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 948 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 949 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 950 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 951 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 952 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 953 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 954 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 955 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 956 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 957 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 958 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 959 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 960 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 961 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 962 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 963 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 964 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 965 train_loss: 0.4756, val_loss: 0.5359\n",
      "Epoch 966 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 967 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 968 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 969 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 970 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 971 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 972 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 973 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 974 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 975 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 976 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 977 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 978 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 979 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 980 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 981 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 982 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 983 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 984 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 985 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 986 train_loss: 0.4756, val_loss: 0.5356\n",
      "Epoch 987 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 988 train_loss: 0.4756, val_loss: 0.5355\n",
      "Epoch 989 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 990 train_loss: 0.4756, val_loss: 0.5355\n",
      "Epoch 991 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 992 train_loss: 0.4756, val_loss: 0.5355\n",
      "Epoch 993 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 994 train_loss: 0.4756, val_loss: 0.5355\n",
      "Epoch 995 train_loss: 0.4756, val_loss: 0.5358\n",
      "Epoch 996 train_loss: 0.4756, val_loss: 0.5355\n",
      "Epoch 997 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 998 train_loss: 0.4756, val_loss: 0.5355\n",
      "Epoch 999 train_loss: 0.4756, val_loss: 0.5357\n",
      "Epoch 1000 train_loss: 0.4756, val_loss: 0.5355\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, 1000 + 1):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # training\n",
    "    for data in dataset_train:\n",
    "        loss = model.train_step(data)\n",
    "        train_loss.append(loss.numpy())\n",
    "\n",
    "    # validating\n",
    "    for data in dataset_val:\n",
    "        loss = model.val_step(data)\n",
    "        val_loss.append(loss.numpy())\n",
    "\n",
    "    # record losses\n",
    "    avg_train_loss = np.mean(train_loss)\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # print losses\n",
    "    print(f'Epoch {epoch} train_loss: {avg_train_loss:.4f}, val_loss: {avg_val_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-GPU-Eric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
