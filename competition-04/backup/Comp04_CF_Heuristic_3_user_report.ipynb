{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DataLab Cup 4: Recommender Systems</center>\n",
    "<center>Shan-Hung Wu & DataLab</center>\n",
    "<center>Fall 2023</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team: 陳瑜旋轉陳玟旋轉陳瑜旋\n",
    "\n",
    "Team Member: 111501538 劉杰閎、111062588 陳玟璇、111062697 吳律穎"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform: [Kaggle](https://www.kaggle.com/t/b06e248a3827434f80c4fdc6009d5fe0)\n",
    "\n",
    "Please download the dataset and the environment source code from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 04:05:22.825782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation.environment import TrainingEnvironment, TestingEnvironment\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 04:05:25.905052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.905260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.911081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.911291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.911487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.911629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.912428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 04:05:25.913216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.913396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:25.913523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:26.402591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:26.402813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:26.402952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-16 04:05:26.403087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9240 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Select GPU number 1\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official hyperparameters for this competition (do not modify)\n",
    "N_TRAIN_USERS = 1000\n",
    "N_TEST_USERS = 2000\n",
    "N_ITEMS = 209527\n",
    "HORIZON = 2000\n",
    "TEST_EPISODES = 5\n",
    "SLATE_SIZE = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "USER_DATA = os.path.join('dataset', 'user_data.json')\n",
    "ITEM_DATA = os.path.join('dataset', 'item_data.json')\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_PATH = os.path.join('output', 'output.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[42558, 65272, 13353]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[146057, 195688, 143652]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[67551, 85247, 33714]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[116097, 192703, 103229]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[68756, 140123, 135289]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>[95090, 131393, 130239]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>[2360, 147130, 8145]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>[99794, 138694, 157888]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>[55561, 60372, 51442]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>[125409, 77906, 124792]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   history\n",
       "0           0     [42558, 65272, 13353]\n",
       "1           1  [146057, 195688, 143652]\n",
       "2           2     [67551, 85247, 33714]\n",
       "3           3  [116097, 192703, 103229]\n",
       "4           4   [68756, 140123, 135289]\n",
       "...       ...                       ...\n",
       "1995     1995   [95090, 131393, 130239]\n",
       "1996     1996      [2360, 147130, 8145]\n",
       "1997     1997   [99794, 138694, 157888]\n",
       "1998     1998     [55561, 60372, 51442]\n",
       "1999     1999   [125409, 77906, 124792]\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user = pd.read_json(USER_DATA, lines=True)\n",
    "df_user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>209522</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>209523</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>209524</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>209525</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>209526</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id                                           headline  \\\n",
       "0             0  Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1             1  American Airlines Flyer Charged, Banned For Li...   \n",
       "2             2  23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3             3  The Funniest Tweets From Parents This Week (Se...   \n",
       "4             4  Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "...         ...                                                ...   \n",
       "209522   209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "209523   209523  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "209524   209524  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "209525   209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "209526   209526  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                                        short_description  \n",
       "0       Health experts said it is too early to predict...  \n",
       "1       He was subdued by passengers and crew when he ...  \n",
       "2       \"Until you have a dog you don't understand wha...  \n",
       "3       \"Accidentally put grown-up toothpaste on my to...  \n",
       "4       Amy Cooper accused investment firm Franklin Te...  \n",
       "...                                                   ...  \n",
       "209522  Verizon Wireless and AT&T are already promotin...  \n",
       "209523  Afterward, Azarenka, more effusive with the pr...  \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...  \n",
       "209525  CORRECTION: An earlier version of this story i...  \n",
       "209526  The five-time all-star center tore into his te...  \n",
       "\n",
       "[209527 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item = pd.read_json(ITEM_DATA, lines=True)\n",
    "df_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embedding from item descriptions\n",
    "\n",
    "在這裡我們針對每一個item的描述，使用`SentenceTransformer`去做Text Embedding，可以得到每一個item都有headline的embedding跟short_description的embedding，兩者的維度都是768。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "      <th>headline_embeddings</th>\n",
       "      <th>short_description_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>[-0.054995973, 0.10514701, 0.0009537986, -0.07...</td>\n",
       "      <td>[0.04689467, 0.089309394, -0.018575395, -0.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>[-0.020863444, 0.011131575, 0.0013632453, -0.0...</td>\n",
       "      <td>[0.017128233, -0.0062120855, 0.015252358, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>[0.017761054, 0.053476874, 6.918786e-05, -0.03...</td>\n",
       "      <td>[0.10238154, 0.07736524, 0.0020822838, -0.0614...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>[-0.0029250348, 0.01137404, 0.0045979875, -0.0...</td>\n",
       "      <td>[0.04334459, 0.056244634, 0.0071496996, -0.057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>[-0.0049342206, 0.053551663, 0.027952224, -0.0...</td>\n",
       "      <td>[-0.0066743735, 0.03416268, -0.00058029604, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>209522</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>[0.041218493, -0.007820907, -0.01887703, -0.02...</td>\n",
       "      <td>[-0.029524302, -0.0045847334, -0.054970894, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>209523</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>[-0.047861934, -0.027825285, -0.0048302715, -0...</td>\n",
       "      <td>[0.03547541, -0.027677324, 0.019167567, -0.007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>209524</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>[-0.0816778, 0.022369152, 0.027179016, 0.02018...</td>\n",
       "      <td>[-0.020275101, 0.10664522, -0.007810726, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>209525</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>[-0.04274766, 0.12479968, -0.047635496, -0.057...</td>\n",
       "      <td>[0.044633802, 0.014033731, -0.004920267, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>209526</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>[-0.026855215, -0.03475871, -0.0026978385, -0....</td>\n",
       "      <td>[-0.0026186472, -0.025399072, 0.01087954, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id                                           headline  \\\n",
       "0             0  Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1             1  American Airlines Flyer Charged, Banned For Li...   \n",
       "2             2  23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3             3  The Funniest Tweets From Parents This Week (Se...   \n",
       "4             4  Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "...         ...                                                ...   \n",
       "209522   209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "209523   209523  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "209524   209524  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "209525   209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "209526   209526  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       Health experts said it is too early to predict...   \n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                                      headline_embeddings  \\\n",
       "0       [-0.054995973, 0.10514701, 0.0009537986, -0.07...   \n",
       "1       [-0.020863444, 0.011131575, 0.0013632453, -0.0...   \n",
       "2       [0.017761054, 0.053476874, 6.918786e-05, -0.03...   \n",
       "3       [-0.0029250348, 0.01137404, 0.0045979875, -0.0...   \n",
       "4       [-0.0049342206, 0.053551663, 0.027952224, -0.0...   \n",
       "...                                                   ...   \n",
       "209522  [0.041218493, -0.007820907, -0.01887703, -0.02...   \n",
       "209523  [-0.047861934, -0.027825285, -0.0048302715, -0...   \n",
       "209524  [-0.0816778, 0.022369152, 0.027179016, 0.02018...   \n",
       "209525  [-0.04274766, 0.12479968, -0.047635496, -0.057...   \n",
       "209526  [-0.026855215, -0.03475871, -0.0026978385, -0....   \n",
       "\n",
       "                             short_description_embeddings  \n",
       "0       [0.04689467, 0.089309394, -0.018575395, -0.029...  \n",
       "1       [0.017128233, -0.0062120855, 0.015252358, 0.02...  \n",
       "2       [0.10238154, 0.07736524, 0.0020822838, -0.0614...  \n",
       "3       [0.04334459, 0.056244634, 0.0071496996, -0.057...  \n",
       "4       [-0.0066743735, 0.03416268, -0.00058029604, 0....  \n",
       "...                                                   ...  \n",
       "209522  [-0.029524302, -0.0045847334, -0.054970894, -0...  \n",
       "209523  [0.03547541, -0.027677324, 0.019167567, -0.007...  \n",
       "209524  [-0.020275101, 0.10664522, -0.007810726, -0.01...  \n",
       "209525  [0.044633802, 0.014033731, -0.004920267, -0.02...  \n",
       "209526  [-0.0026186472, -0.025399072, 0.01087954, -0.0...  \n",
       "\n",
       "[209527 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = './dataset/'\n",
    "if os.path.exists(dataset_dir + 'train_data_embedding.pkl'):\n",
    "    df_item_train = pd.read_pickle(dataset_dir + 'train_data_embedding.pkl')\n",
    "else:\n",
    "    sbert = SentenceTransformer('all-mpnet-base-v2')\n",
    "    df_item_train = pd.read_json(ITEM_DATA, lines=True)\n",
    "    df_item_train['headline_embeddings'] = df_item_train['headline'].apply(lambda x: sbert.encode(x))\n",
    "    df_item_train['short_description_embeddings'] = df_item_train['short_description'].apply(lambda x: sbert.encode(x))\n",
    "    df_item_train.to_pickle(dataset_dir + 'train_data_embedding.pkl')\n",
    "\n",
    "df_item_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating User Embedding dataframe\n",
    "\n",
    "+ 我們會建立一個 User Embedding dataframe來代表每一個User的feature\n",
    "+ 計算的方式就是透過每一個user之前點過哪三個item，我們去拿那三個item的embedding出來取平均，就當作這個user的embedding，最後一個column是把headline跟short description concat起來。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USERS = 2000\n",
    "N_ITEMS = 209527\n",
    "EMBEDDING_DIM = 768\n",
    "HISTORY_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dataset_dir + 'user_embedding.pkl'):\n",
    "    df_user_embedding = pd.read_pickle(dataset_dir+'user_embedding.pkl')\n",
    "else:\n",
    "    df_user_embedding_list = []\n",
    "    for user in range(N_USERS):\n",
    "        # print(df_user.iloc[user])\n",
    "        sum_headline = tf.zeros(shape=(EMBEDDING_DIM,)) # since all embeddings are (768,)\n",
    "        sum_short_description = tf.zeros(shape=(EMBEDDING_DIM,)) # since all embeddings are (768,)\n",
    "        for item in df_user.iloc[user][\"history\"]:\n",
    "            headline_tensor = tf.convert_to_tensor(df_item_train.iloc[item][\"headline_embeddings\"])\n",
    "            short_description_tensor = tf.convert_to_tensor(df_item_train.iloc[item][\"short_description_embeddings\"])\n",
    "            sum_headline += headline_tensor\n",
    "            sum_short_description += short_description_tensor\n",
    "        sum_headline = tf.divide(sum_headline, HISTORY_SIZE)\n",
    "        sum_short_description = tf.divide(sum_short_description, HISTORY_SIZE)\n",
    "        concat_embedding = tf.concat([sum_headline, sum_short_description],axis=0)\n",
    "        df_user_embedding_list.append([user, sum_headline.numpy(), sum_short_description.numpy(), concat_embedding.numpy()])\n",
    "    df_user_embedding = pd.DataFrame(df_user_embedding_list, columns = ['user_id', 'headline_embedding', 'short_description_embedding','concat_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                                                                        0\n",
      "headline_embedding             [0.038156908, 0.041387293, -0.004624029, -0.02...\n",
      "short_description_embedding    [0.03716896, 0.0512002, -0.025162613, -0.01830...\n",
      "concat_embeddings              [0.038156908, 0.041387293, -0.004624029, -0.02...\n",
      "Name: 0, dtype: object\n",
      "0.03716896\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>headline_embedding</th>\n",
       "      <th>short_description_embedding</th>\n",
       "      <th>concat_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.038156908, 0.041387293, -0.004624029, -0.02...</td>\n",
       "      <td>[0.03716896, 0.0512002, -0.025162613, -0.01830...</td>\n",
       "      <td>[0.038156908, 0.041387293, -0.004624029, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01718974, 0.04380578, -0.0033005949, 0.026...</td>\n",
       "      <td>[-0.022690356, 0.041603807, -0.009130617, -0.0...</td>\n",
       "      <td>[-0.01718974, 0.04380578, -0.0033005949, 0.026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0044823308, -0.017107317, -0.038572405, -0...</td>\n",
       "      <td>[0.03541447, 0.021701857, -0.016264068, -0.025...</td>\n",
       "      <td>[-0.0044823308, -0.017107317, -0.038572405, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.03414116, 0.035626348, -0.024177575, 0.043...</td>\n",
       "      <td>[-0.032475274, 0.034354758, -0.0064822477, 0.0...</td>\n",
       "      <td>[-0.03414116, 0.035626348, -0.024177575, 0.043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.0039870082, 0.082041055, -0.01948834, -0.0...</td>\n",
       "      <td>[0.009200389, 0.0539891, -0.026606128, -0.0117...</td>\n",
       "      <td>[-0.0039870082, 0.082041055, -0.01948834, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                 headline_embedding  \\\n",
       "0        0  [0.038156908, 0.041387293, -0.004624029, -0.02...   \n",
       "1        1  [-0.01718974, 0.04380578, -0.0033005949, 0.026...   \n",
       "2        2  [-0.0044823308, -0.017107317, -0.038572405, -0...   \n",
       "3        3  [-0.03414116, 0.035626348, -0.024177575, 0.043...   \n",
       "4        4  [-0.0039870082, 0.082041055, -0.01948834, -0.0...   \n",
       "\n",
       "                         short_description_embedding  \\\n",
       "0  [0.03716896, 0.0512002, -0.025162613, -0.01830...   \n",
       "1  [-0.022690356, 0.041603807, -0.009130617, -0.0...   \n",
       "2  [0.03541447, 0.021701857, -0.016264068, -0.025...   \n",
       "3  [-0.032475274, 0.034354758, -0.0064822477, 0.0...   \n",
       "4  [0.009200389, 0.0539891, -0.026606128, -0.0117...   \n",
       "\n",
       "                                   concat_embeddings  \n",
       "0  [0.038156908, 0.041387293, -0.004624029, -0.02...  \n",
       "1  [-0.01718974, 0.04380578, -0.0033005949, 0.026...  \n",
       "2  [-0.0044823308, -0.017107317, -0.038572405, -0...  \n",
       "3  [-0.03414116, 0.035626348, -0.024177575, 0.043...  \n",
       "4  [-0.0039870082, 0.082041055, -0.01948834, -0.0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df_user_embedding.iloc[0])\n",
    "print(df_user_embedding.iloc[0][\"concat_embeddings\"][768])\n",
    "display(df_user_embedding.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Item embedding dataframe\n",
    "\n",
    "那我們也重新去建立一個item embedding，丟棄調原本對item的description，最後一個column是把headline跟short description concat起來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(dataset_dir+'item_embedding.pkl')):\n",
    "    df_item_embedding = pd.read_pickle(dataset_dir+'item_embedding.pkl')\n",
    "else:\n",
    "    item_embedding_list = []\n",
    "    for item in range(len(df_item_train)):\n",
    "        headline_tensor = tf.convert_to_tensor(df_item_train.iloc[item][\"headline_embeddings\"])\n",
    "        short_description_tensor = tf.convert_to_tensor(df_item_train.iloc[item][\"short_description_embeddings\"])\n",
    "        concat_embedding = tf.concat([headline_tensor, short_description_tensor],axis=0)\n",
    "        item_embedding_list.append([item, headline_tensor.numpy(), short_description_tensor.numpy(), concat_embedding.numpy()])\n",
    "    df_item_embedding = pd.DataFrame(item_embedding_list, columns=[\"item_id\",\"headline_embeddings\",\"short_description_embeddings\",\"concat_embeddings\"])\n",
    "    df_item_embedding.to_pickle(dataset_dir + 'item_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id                                                                         0\n",
      "headline_embeddings             [-0.054995973, 0.10514701, 0.0009537986, -0.07...\n",
      "short_description_embeddings    [0.04689467, 0.089309394, -0.018575395, -0.029...\n",
      "concat_embeddings               [-0.054995973, 0.10514701, 0.0009537986, -0.07...\n",
      "Name: 0, dtype: object\n",
      "0.04689467\n",
      "len of item dataframe is  209527\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>headline_embeddings</th>\n",
       "      <th>short_description_embeddings</th>\n",
       "      <th>concat_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.054995973, 0.10514701, 0.0009537986, -0.07...</td>\n",
       "      <td>[0.04689467, 0.089309394, -0.018575395, -0.029...</td>\n",
       "      <td>[-0.054995973, 0.10514701, 0.0009537986, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.020863444, 0.011131575, 0.0013632453, -0.0...</td>\n",
       "      <td>[0.017128233, -0.0062120855, 0.015252358, 0.02...</td>\n",
       "      <td>[-0.020863444, 0.011131575, 0.0013632453, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.017761054, 0.053476874, 6.918786e-05, -0.03...</td>\n",
       "      <td>[0.10238154, 0.07736524, 0.0020822838, -0.0614...</td>\n",
       "      <td>[0.017761054, 0.053476874, 6.918786e-05, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.0029250348, 0.01137404, 0.0045979875, -0.0...</td>\n",
       "      <td>[0.04334459, 0.056244634, 0.0071496996, -0.057...</td>\n",
       "      <td>[-0.0029250348, 0.01137404, 0.0045979875, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.0049342206, 0.053551663, 0.027952224, -0.0...</td>\n",
       "      <td>[-0.0066743735, 0.03416268, -0.00058029604, 0....</td>\n",
       "      <td>[-0.0049342206, 0.053551663, 0.027952224, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                headline_embeddings  \\\n",
       "0        0  [-0.054995973, 0.10514701, 0.0009537986, -0.07...   \n",
       "1        1  [-0.020863444, 0.011131575, 0.0013632453, -0.0...   \n",
       "2        2  [0.017761054, 0.053476874, 6.918786e-05, -0.03...   \n",
       "3        3  [-0.0029250348, 0.01137404, 0.0045979875, -0.0...   \n",
       "4        4  [-0.0049342206, 0.053551663, 0.027952224, -0.0...   \n",
       "\n",
       "                        short_description_embeddings  \\\n",
       "0  [0.04689467, 0.089309394, -0.018575395, -0.029...   \n",
       "1  [0.017128233, -0.0062120855, 0.015252358, 0.02...   \n",
       "2  [0.10238154, 0.07736524, 0.0020822838, -0.0614...   \n",
       "3  [0.04334459, 0.056244634, 0.0071496996, -0.057...   \n",
       "4  [-0.0066743735, 0.03416268, -0.00058029604, 0....   \n",
       "\n",
       "                                   concat_embeddings  \n",
       "0  [-0.054995973, 0.10514701, 0.0009537986, -0.07...  \n",
       "1  [-0.020863444, 0.011131575, 0.0013632453, -0.0...  \n",
       "2  [0.017761054, 0.053476874, 6.918786e-05, -0.03...  \n",
       "3  [-0.0029250348, 0.01137404, 0.0045979875, -0.0...  \n",
       "4  [-0.0049342206, 0.053551663, 0.027952224, -0.0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df_item_embedding.iloc[0])\n",
    "print(df_item_embedding.iloc[0][\"concat_embeddings\"][768])\n",
    "print(\"len of item dataframe is \", len(df_item_embedding))\n",
    "display(df_item_embedding.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create user-user similarity matrix \n",
    "\n",
    "為了考慮user與user之間的相似度，我們需要用到之前算好的user embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>headline_embedding</th>\n",
       "      <th>short_description_embedding</th>\n",
       "      <th>concat_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.038156908, 0.041387293, -0.004624029, -0.02...</td>\n",
       "      <td>[0.03716896, 0.0512002, -0.025162613, -0.01830...</td>\n",
       "      <td>[0.038156908, 0.041387293, -0.004624029, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.01718974, 0.04380578, -0.0033005949, 0.026...</td>\n",
       "      <td>[-0.022690356, 0.041603807, -0.009130617, -0.0...</td>\n",
       "      <td>[-0.01718974, 0.04380578, -0.0033005949, 0.026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.0044823308, -0.017107317, -0.038572405, -0...</td>\n",
       "      <td>[0.03541447, 0.021701857, -0.016264068, -0.025...</td>\n",
       "      <td>[-0.0044823308, -0.017107317, -0.038572405, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.03414116, 0.035626348, -0.024177575, 0.043...</td>\n",
       "      <td>[-0.032475274, 0.034354758, -0.0064822477, 0.0...</td>\n",
       "      <td>[-0.03414116, 0.035626348, -0.024177575, 0.043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.0039870082, 0.082041055, -0.01948834, -0.0...</td>\n",
       "      <td>[0.009200389, 0.0539891, -0.026606128, -0.0117...</td>\n",
       "      <td>[-0.0039870082, 0.082041055, -0.01948834, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>[0.007958271, 0.06905828, -0.012985666, 0.0113...</td>\n",
       "      <td>[-0.005231034, 0.03755425, -0.013397035, -0.01...</td>\n",
       "      <td>[0.007958271, 0.06905828, -0.012985666, 0.0113...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>[-0.036823038, 0.03161138, -0.017325308, 0.003...</td>\n",
       "      <td>[-0.010244309, -0.01270321, 0.00085817085, -0....</td>\n",
       "      <td>[-0.036823038, 0.03161138, -0.017325308, 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>[0.02073842, 0.056454603, 0.000992029, 0.00940...</td>\n",
       "      <td>[0.012727796, 0.0049337894, -0.012769024, 0.02...</td>\n",
       "      <td>[0.02073842, 0.056454603, 0.000992029, 0.00940...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>[0.016565206, 0.06024626, -0.0010410805, -0.01...</td>\n",
       "      <td>[0.025898555, 0.046939295, -0.02147156, -0.017...</td>\n",
       "      <td>[0.016565206, 0.06024626, -0.0010410805, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>[0.005492252, 0.064615436, -0.0025653813, 0.01...</td>\n",
       "      <td>[0.0068310047, 0.07291591, -0.0014106488, 0.00...</td>\n",
       "      <td>[0.005492252, 0.064615436, -0.0025653813, 0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                                 headline_embedding  \\\n",
       "0           0  [0.038156908, 0.041387293, -0.004624029, -0.02...   \n",
       "1           1  [-0.01718974, 0.04380578, -0.0033005949, 0.026...   \n",
       "2           2  [-0.0044823308, -0.017107317, -0.038572405, -0...   \n",
       "3           3  [-0.03414116, 0.035626348, -0.024177575, 0.043...   \n",
       "4           4  [-0.0039870082, 0.082041055, -0.01948834, -0.0...   \n",
       "...       ...                                                ...   \n",
       "1995     1995  [0.007958271, 0.06905828, -0.012985666, 0.0113...   \n",
       "1996     1996  [-0.036823038, 0.03161138, -0.017325308, 0.003...   \n",
       "1997     1997  [0.02073842, 0.056454603, 0.000992029, 0.00940...   \n",
       "1998     1998  [0.016565206, 0.06024626, -0.0010410805, -0.01...   \n",
       "1999     1999  [0.005492252, 0.064615436, -0.0025653813, 0.01...   \n",
       "\n",
       "                            short_description_embedding  \\\n",
       "0     [0.03716896, 0.0512002, -0.025162613, -0.01830...   \n",
       "1     [-0.022690356, 0.041603807, -0.009130617, -0.0...   \n",
       "2     [0.03541447, 0.021701857, -0.016264068, -0.025...   \n",
       "3     [-0.032475274, 0.034354758, -0.0064822477, 0.0...   \n",
       "4     [0.009200389, 0.0539891, -0.026606128, -0.0117...   \n",
       "...                                                 ...   \n",
       "1995  [-0.005231034, 0.03755425, -0.013397035, -0.01...   \n",
       "1996  [-0.010244309, -0.01270321, 0.00085817085, -0....   \n",
       "1997  [0.012727796, 0.0049337894, -0.012769024, 0.02...   \n",
       "1998  [0.025898555, 0.046939295, -0.02147156, -0.017...   \n",
       "1999  [0.0068310047, 0.07291591, -0.0014106488, 0.00...   \n",
       "\n",
       "                                      concat_embeddings  \n",
       "0     [0.038156908, 0.041387293, -0.004624029, -0.02...  \n",
       "1     [-0.01718974, 0.04380578, -0.0033005949, 0.026...  \n",
       "2     [-0.0044823308, -0.017107317, -0.038572405, -0...  \n",
       "3     [-0.03414116, 0.035626348, -0.024177575, 0.043...  \n",
       "4     [-0.0039870082, 0.082041055, -0.01948834, -0.0...  \n",
       "...                                                 ...  \n",
       "1995  [0.007958271, 0.06905828, -0.012985666, 0.0113...  \n",
       "1996  [-0.036823038, 0.03161138, -0.017325308, 0.003...  \n",
       "1997  [0.02073842, 0.056454603, 0.000992029, 0.00940...  \n",
       "1998  [0.016565206, 0.06024626, -0.0010410805, -0.01...  \n",
       "1999  [0.005492252, 0.064615436, -0.0025653813, 0.01...  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_user_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於test user沒辦法透過收集資料來知道他們的喜好，因此我們會先透過上面的user embeding計算test user跟各個train user的cosine similarity。這樣在推薦item給test user的時候，就可以找出哪些train user跟他比較相近，就可以推薦這個test user那些train user會喜歡的東西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(dataset_dir + 'user_user_similarity_matrix.pkl')):\n",
    "    df_user_user_similarity_matrix = pd.read_pickle(dataset_dir + 'user_user_similarity_matrix.pkl')\n",
    "else:\n",
    "    user_user_similarity_matrix = []\n",
    "    for train_user in range(0,1000):\n",
    "        row_similarity_list = []\n",
    "        for test_user in range(1000,2000):\n",
    "            cosine_similarity = (1-spatial.distance.cosine(df_user_embedding.iloc[train_user][\"concat_embeddings\"], df_user_embedding.iloc[test_user][\"concat_embeddings\"]))\n",
    "            row_similarity_list.append(cosine_similarity)\n",
    "        user_user_similarity_matrix.append(row_similarity_list)\n",
    "    # print(user_user_similarity_matrix)\n",
    "    columns_names = list(range(1000,2000))\n",
    "    df_user_user_similarity_matrix = pd.DataFrame(user_user_similarity_matrix, columns = columns_names)\n",
    "    df_user_user_similarity_matrix.to_pickle(dataset_dir + 'user_user_similarity_matrix.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1000</th>\n",
       "      <th>1001</th>\n",
       "      <th>1002</th>\n",
       "      <th>1003</th>\n",
       "      <th>1004</th>\n",
       "      <th>1005</th>\n",
       "      <th>1006</th>\n",
       "      <th>1007</th>\n",
       "      <th>1008</th>\n",
       "      <th>1009</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.281648</td>\n",
       "      <td>0.208695</td>\n",
       "      <td>0.125609</td>\n",
       "      <td>0.179470</td>\n",
       "      <td>0.183822</td>\n",
       "      <td>0.109009</td>\n",
       "      <td>0.052345</td>\n",
       "      <td>0.202675</td>\n",
       "      <td>0.335739</td>\n",
       "      <td>0.217125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173621</td>\n",
       "      <td>0.222536</td>\n",
       "      <td>0.221223</td>\n",
       "      <td>0.189922</td>\n",
       "      <td>0.258909</td>\n",
       "      <td>0.221459</td>\n",
       "      <td>0.169270</td>\n",
       "      <td>0.253004</td>\n",
       "      <td>0.236725</td>\n",
       "      <td>0.312044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.107756</td>\n",
       "      <td>0.157925</td>\n",
       "      <td>0.171894</td>\n",
       "      <td>0.191783</td>\n",
       "      <td>0.215627</td>\n",
       "      <td>0.263724</td>\n",
       "      <td>0.066528</td>\n",
       "      <td>0.048420</td>\n",
       "      <td>0.140302</td>\n",
       "      <td>0.219080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188666</td>\n",
       "      <td>0.127834</td>\n",
       "      <td>0.170375</td>\n",
       "      <td>0.164114</td>\n",
       "      <td>0.132382</td>\n",
       "      <td>0.155832</td>\n",
       "      <td>0.172978</td>\n",
       "      <td>0.137667</td>\n",
       "      <td>0.036835</td>\n",
       "      <td>0.184401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.340798</td>\n",
       "      <td>0.104089</td>\n",
       "      <td>0.117053</td>\n",
       "      <td>0.121768</td>\n",
       "      <td>0.192938</td>\n",
       "      <td>0.217003</td>\n",
       "      <td>-0.007815</td>\n",
       "      <td>0.082012</td>\n",
       "      <td>0.093833</td>\n",
       "      <td>0.201811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162832</td>\n",
       "      <td>0.347783</td>\n",
       "      <td>0.093973</td>\n",
       "      <td>0.229857</td>\n",
       "      <td>0.219791</td>\n",
       "      <td>0.337218</td>\n",
       "      <td>-0.006569</td>\n",
       "      <td>0.166411</td>\n",
       "      <td>0.199871</td>\n",
       "      <td>0.136065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.152984</td>\n",
       "      <td>0.204095</td>\n",
       "      <td>0.054092</td>\n",
       "      <td>0.151933</td>\n",
       "      <td>0.109114</td>\n",
       "      <td>0.051870</td>\n",
       "      <td>0.128989</td>\n",
       "      <td>0.186338</td>\n",
       "      <td>0.399219</td>\n",
       "      <td>0.216707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244051</td>\n",
       "      <td>0.087133</td>\n",
       "      <td>0.480642</td>\n",
       "      <td>0.195987</td>\n",
       "      <td>0.244529</td>\n",
       "      <td>0.177108</td>\n",
       "      <td>0.142491</td>\n",
       "      <td>0.211308</td>\n",
       "      <td>-0.009201</td>\n",
       "      <td>0.387058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.410860</td>\n",
       "      <td>0.238368</td>\n",
       "      <td>0.188938</td>\n",
       "      <td>0.215626</td>\n",
       "      <td>0.339893</td>\n",
       "      <td>0.351212</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.204705</td>\n",
       "      <td>0.207856</td>\n",
       "      <td>0.335223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293352</td>\n",
       "      <td>0.420971</td>\n",
       "      <td>0.161138</td>\n",
       "      <td>0.279038</td>\n",
       "      <td>0.344157</td>\n",
       "      <td>0.386043</td>\n",
       "      <td>0.085055</td>\n",
       "      <td>0.218188</td>\n",
       "      <td>0.275574</td>\n",
       "      <td>0.251951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.318691</td>\n",
       "      <td>0.269468</td>\n",
       "      <td>0.119631</td>\n",
       "      <td>0.186885</td>\n",
       "      <td>0.302532</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>0.058353</td>\n",
       "      <td>0.258401</td>\n",
       "      <td>0.215519</td>\n",
       "      <td>0.326197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224176</td>\n",
       "      <td>0.286827</td>\n",
       "      <td>0.162822</td>\n",
       "      <td>0.274992</td>\n",
       "      <td>0.254537</td>\n",
       "      <td>0.387950</td>\n",
       "      <td>0.023907</td>\n",
       "      <td>0.317955</td>\n",
       "      <td>0.197057</td>\n",
       "      <td>0.175260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.246249</td>\n",
       "      <td>0.342056</td>\n",
       "      <td>0.188454</td>\n",
       "      <td>0.261937</td>\n",
       "      <td>0.381595</td>\n",
       "      <td>0.282175</td>\n",
       "      <td>0.023506</td>\n",
       "      <td>0.282524</td>\n",
       "      <td>0.276801</td>\n",
       "      <td>0.293508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296109</td>\n",
       "      <td>0.172917</td>\n",
       "      <td>0.195978</td>\n",
       "      <td>0.312822</td>\n",
       "      <td>0.194241</td>\n",
       "      <td>0.184523</td>\n",
       "      <td>0.052026</td>\n",
       "      <td>0.216403</td>\n",
       "      <td>0.212665</td>\n",
       "      <td>0.304270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.253551</td>\n",
       "      <td>0.193244</td>\n",
       "      <td>0.278929</td>\n",
       "      <td>0.220450</td>\n",
       "      <td>0.320774</td>\n",
       "      <td>0.388695</td>\n",
       "      <td>0.158398</td>\n",
       "      <td>0.211402</td>\n",
       "      <td>0.324682</td>\n",
       "      <td>0.186920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220253</td>\n",
       "      <td>0.273932</td>\n",
       "      <td>0.362760</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.410991</td>\n",
       "      <td>0.453089</td>\n",
       "      <td>0.130172</td>\n",
       "      <td>0.230232</td>\n",
       "      <td>0.169021</td>\n",
       "      <td>0.406711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.232465</td>\n",
       "      <td>0.211833</td>\n",
       "      <td>0.121255</td>\n",
       "      <td>0.133941</td>\n",
       "      <td>0.225623</td>\n",
       "      <td>0.214997</td>\n",
       "      <td>0.058819</td>\n",
       "      <td>0.195395</td>\n",
       "      <td>0.183076</td>\n",
       "      <td>0.203365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160095</td>\n",
       "      <td>0.239532</td>\n",
       "      <td>0.179513</td>\n",
       "      <td>0.185097</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.376211</td>\n",
       "      <td>0.013491</td>\n",
       "      <td>0.250970</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>0.201342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.388990</td>\n",
       "      <td>0.280651</td>\n",
       "      <td>0.182958</td>\n",
       "      <td>0.279766</td>\n",
       "      <td>0.207126</td>\n",
       "      <td>0.254176</td>\n",
       "      <td>-0.036328</td>\n",
       "      <td>0.122361</td>\n",
       "      <td>0.082163</td>\n",
       "      <td>0.327857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273888</td>\n",
       "      <td>0.220284</td>\n",
       "      <td>0.094534</td>\n",
       "      <td>0.209046</td>\n",
       "      <td>0.125721</td>\n",
       "      <td>0.165480</td>\n",
       "      <td>0.060655</td>\n",
       "      <td>0.230642</td>\n",
       "      <td>0.180086</td>\n",
       "      <td>0.159151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1000      1001      1002      1003      1004      1005      1006  \\\n",
       "0    0.281648  0.208695  0.125609  0.179470  0.183822  0.109009  0.052345   \n",
       "1    0.107756  0.157925  0.171894  0.191783  0.215627  0.263724  0.066528   \n",
       "2    0.340798  0.104089  0.117053  0.121768  0.192938  0.217003 -0.007815   \n",
       "3    0.152984  0.204095  0.054092  0.151933  0.109114  0.051870  0.128989   \n",
       "4    0.410860  0.238368  0.188938  0.215626  0.339893  0.351212  0.086864   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.318691  0.269468  0.119631  0.186885  0.302532  0.243200  0.058353   \n",
       "996  0.246249  0.342056  0.188454  0.261937  0.381595  0.282175  0.023506   \n",
       "997  0.253551  0.193244  0.278929  0.220450  0.320774  0.388695  0.158398   \n",
       "998  0.232465  0.211833  0.121255  0.133941  0.225623  0.214997  0.058819   \n",
       "999  0.388990  0.280651  0.182958  0.279766  0.207126  0.254176 -0.036328   \n",
       "\n",
       "         1007      1008      1009  ...      1990      1991      1992  \\\n",
       "0    0.202675  0.335739  0.217125  ...  0.173621  0.222536  0.221223   \n",
       "1    0.048420  0.140302  0.219080  ...  0.188666  0.127834  0.170375   \n",
       "2    0.082012  0.093833  0.201811  ...  0.162832  0.347783  0.093973   \n",
       "3    0.186338  0.399219  0.216707  ...  0.244051  0.087133  0.480642   \n",
       "4    0.204705  0.207856  0.335223  ...  0.293352  0.420971  0.161138   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.258401  0.215519  0.326197  ...  0.224176  0.286827  0.162822   \n",
       "996  0.282524  0.276801  0.293508  ...  0.296109  0.172917  0.195978   \n",
       "997  0.211402  0.324682  0.186920  ...  0.220253  0.273932  0.362760   \n",
       "998  0.195395  0.183076  0.203365  ...  0.160095  0.239532  0.179513   \n",
       "999  0.122361  0.082163  0.327857  ...  0.273888  0.220284  0.094534   \n",
       "\n",
       "         1993      1994      1995      1996      1997      1998      1999  \n",
       "0    0.189922  0.258909  0.221459  0.169270  0.253004  0.236725  0.312044  \n",
       "1    0.164114  0.132382  0.155832  0.172978  0.137667  0.036835  0.184401  \n",
       "2    0.229857  0.219791  0.337218 -0.006569  0.166411  0.199871  0.136065  \n",
       "3    0.195987  0.244529  0.177108  0.142491  0.211308 -0.009201  0.387058  \n",
       "4    0.279038  0.344157  0.386043  0.085055  0.218188  0.275574  0.251951  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.274992  0.254537  0.387950  0.023907  0.317955  0.197057  0.175260  \n",
       "996  0.312822  0.194241  0.184523  0.052026  0.216403  0.212665  0.304270  \n",
       "997  0.306700  0.410991  0.453089  0.130172  0.230232  0.169021  0.406711  \n",
       "998  0.185097  0.231373  0.376211  0.013491  0.250970  0.063096  0.201342  \n",
       "999  0.209046  0.125721  0.165480  0.060655  0.230642  0.180086  0.159151  \n",
       "\n",
       "[1000 rows x 1000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_user_user_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Environments (Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在執行testing enviroment之前，我們要先讀取之前在training environment收集到的data做為推薦的依據。比較特別的是，我們蒐集資料的方法是，我們只會蒐集user真正有點擊的東西，而不會把那些不點擊的東西也記錄下來。原本也有試過都收集下來，然後送給最後面我們嘗試過的model訓練，但是這樣反而會造成model的偏差，因此最後就只收集有點擊的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Experience Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取經驗資料的時候，我們會用一個user_item_info的dictionary，紀錄每個user點擊過哪些item以及這些item被點擊的次數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_info = {uid: {} for uid in range(N_TEST_USERS)}  # for every element: {user_id: {item_id: click_count}}\n",
    "\n",
    "with open('./dataset/clicked_ids_output_final.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split(', ')\n",
    "        user_id = int(line[0])\n",
    "        item_id = int(line[1])\n",
    "        \n",
    "        try:\n",
    "            user_item_info[user_id][item_id] += 1\n",
    "        except KeyError:\n",
    "            user_item_info[user_id][item_id] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 3 alike user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同前所述，test user的喜好沒辦法透過蒐集資料的方式來取得，因此我們先用前面算出的user-user similarity matrix來找出test user最接近的三個train user。接下來做正規化，讓這三個train user跟test user相似度加總為1作為權重。然後就可以將這三個train user喜歡的物品及點擊次數乘上對應的權重，得到test user的interest item及點擊次數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure test user is empty dict\n",
    "for uid in range(1000, 2000):\n",
    "    user_item_info[uid] = {}\n",
    "\n",
    "for test_user_id in range(1000, 2000):\n",
    "    # print(\"Origin similarity:\", df_user_item_similarity_latest.iloc[test_user][0])\n",
    "    column_data = df_user_user_similarity_matrix[test_user_id]\n",
    "    top_three_similarity = column_data.nlargest(3) # top 3 alike user and its similarity\n",
    "    \n",
    "    sum_of_similarities = top_three_similarity.sum() # used to normalized\n",
    "    normalized_list = [top_three_similarity.iloc[i]/sum_of_similarities for i in range(3)]\n",
    "    # print(normalized_list, \"\\n\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        alike_user_id = top_three_similarity.index[i]\n",
    "        interest_items = list(user_item_info[alike_user_id].keys())\n",
    "        interest_count = list(user_item_info[alike_user_id].values())\n",
    "        interest_count = [x * normalized_list[i] for x in interest_count]\n",
    "                \n",
    "        for j in range(len(interest_items)):\n",
    "            item_id = interest_items[j]\n",
    "            item_count = interest_count[j]\n",
    "            try:\n",
    "                user_item_info[test_user_id][item_id] += item_count\n",
    "            except KeyError:\n",
    "                user_item_info[test_user_id][item_id] = item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1590.62it/s]\n"
     ]
    }
   ],
   "source": [
    "item_dict = {}\n",
    "item_weights = {}\n",
    "\n",
    "for user_id in tqdm(range(N_TEST_USERS)):\n",
    "    item_dict[user_id] = list(user_item_info[user_id].keys())\n",
    "    interest_count = np.array(list(user_item_info[user_id].values()))\n",
    "    total_count = interest_count.sum()\n",
    "    item_weights[user_id] = interest_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了所有user的interest item以及點擊次數之後，在testing environment遇到一個user，就可以推薦他的interest item給他，而點擊次數越大的會有越高的機會被推薦。另外，由於一個item被某個user點擊之後，這個user在這個episode再次點擊這個item的機率就會大幅下降，因此如果user有點擊，我們會把這個user的這個item的weight設定為0，這樣這個item在這個episode就不會再出現在推薦名單裡了。(Weight在每個episode一開始會先複製一份原始的，因此改變weight不會影響到下個episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 268914it [13:46, 325.39it/s]\n",
      "Testing: 271462it [14:09, 319.64it/s]\n",
      "Testing: 271148it [14:00, 322.46it/s]\n",
      "Testing: 271236it [14:08, 319.56it/s]\n",
      "Testing: 269362it [13:55, 322.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.8039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  avg_score\n",
       "0           0     0.0095\n",
       "1           1     0.8039\n",
       "2           2     0.0243\n",
       "3           3     0.0226\n",
       "4           4     0.0159\n",
       "...       ...        ...\n",
       "1995     1995     0.0029\n",
       "1996     1996     0.0032\n",
       "1997     1997     0.0029\n",
       "1998     1998     0.0028\n",
       "1999     1999     0.0030\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(TESTING):\n",
    "    # Initialize the testing environment\n",
    "    test_env = TestingEnvironment()\n",
    "    scores = []\n",
    "\n",
    "    # The item_ids here is for the random recommender\n",
    "    item_ids = [i for i in range(N_ITEMS)]\n",
    "\n",
    "    # Repeat the testing process for 5 times\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        # [TODO] Load your model weights here (in the beginning of each testing episode)\n",
    "        # [TODO] Code for loading your model weights...\n",
    "        # df_test_similarity = df_user_item_similarity_latest.copy() # reload from train\n",
    "        current_weights = copy.deepcopy(item_weights)\n",
    "        \n",
    "        # Start the testing process\n",
    "        with tqdm(desc='Testing') as pbar:\n",
    "            # Run as long as there exist some active users\n",
    "            while test_env.has_next_state():\n",
    "                # Get the current user id\n",
    "                cur_user = test_env.get_state()\n",
    "\n",
    "                # [TODO] Employ your recommendation policy to generate a slate of 5 distinct items\n",
    "                # [TODO] Code for generating the recommended slate...\n",
    "                # Here we provide a simple random implementation\n",
    "                # slate = random.sample(item_ids, k=SLATE_SIZE)\n",
    "\n",
    "                interest_items = item_dict[cur_user]\n",
    "                weight = current_weights[cur_user]\n",
    "                \n",
    "                slate = random.choices(interest_items, weight, k=5)\n",
    "                while len(np.unique(slate)) != SLATE_SIZE:\n",
    "                    slate = random.choices(interest_items, weight, k=5)\n",
    "    \n",
    "                    \n",
    "                # Get the response of the slate from the environment\n",
    "                clicked_id, in_environment = test_env.get_response(slate)\n",
    "                if (clicked_id != -1):\n",
    "                    weight_idx = interest_items.index(clicked_id)\n",
    "                    current_weights[cur_user][weight_idx] = 0\n",
    "                    \n",
    "\n",
    "                # [TODO] Update your model here (optional)\n",
    "                # [TODO] You can update your model at each step, or perform a batched update after some interval\n",
    "                # [TODO] Code for updating your model...\n",
    "   \n",
    "   \n",
    "                # Update the progress indicator\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Record the score of this testing episode\n",
    "        scores.append(test_env.get_score())\n",
    "\n",
    "        # Reset the testing environment\n",
    "        test_env.reset()\n",
    "\n",
    "        # [TODO] Delete or reset your model weights here (in the end of each testing episode)\n",
    "        # [TODO] Code for deleting your model weights...\n",
    "        # df_test_similarity = df_user_item_similarity.copy()\n",
    "    # Calculate the average scores \n",
    "    avg_scores = [np.average(score) for score in zip(*scores)]\n",
    "\n",
    "    # Generate a DataFrame to output the result in a .csv file\n",
    "    df_result = pd.DataFrame([[user_id, avg_score] for user_id, avg_score in enumerate(avg_scores)], columns=['user_id', 'avg_score'])\n",
    "    df_result.to_csv(OUTPUT_PATH, index=False)\n",
    "    display(df_result)\n",
    "else:\n",
    "    print(\"Not to test this time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test score:135.2122\n",
      "eval metric: 0.9323939\n"
     ]
    }
   ],
   "source": [
    "if(TESTING):\n",
    "    total_score = df_result['avg_score'].sum()\n",
    "    print(f\"Total test score:{total_score}\")\n",
    "    print(f\"eval metric: {1-total_score/2000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models we have tried during the competition\n",
    "\n",
    "### 1. Vanilla FunkSVD\n",
    "* train用FunkSVD的model來當policy network，環境會給予正確的labal，用這個labal與predict出的結果算LOSS，並最小化total LOSS。\n",
    "* 實驗結果：total loss下降了，可是avg score沒有上升，應該是loss訂得不好，不能將沒選到的item的labal都設為0因為有可能他是target item。\n",
    "\n",
    "### 2. Q-learning\n",
    "* 因為感覺這次的比賽題目跟RL的概念蠻像的，因此嘗試用Q-learning來解決這個問題。我們把state定義為user id，action定義為由五個item id的組成的slate，而rewad就是輸入slate到環境所得到的點擊結果，當有item被點擊時reward設為1，否則reward為-1。\n",
    "* 實驗結果：跑了幾個episode之後發現cumulated reward都沒有上升的趨勢，而且跑一個episode要跑很久，推測可能是action set太大，因此就放棄了這個方法。\n",
    "\n",
    "### 3. DNN-based recommender\n",
    "* 用了幾個embedding layer還有dense layer建構dnn-based model，讓他可以輸入user id和item id然後預測評分，評分範圍是1~5，1代表極度沒興趣，5代表超級有興趣。\n",
    "* 實驗結果：原本想說如果預測的準，就可以知道用戶對於各個item的評分，然後就可以在環境推薦評價5分的item給使用者。但是可能是model架得不好，或是收集到的資料還是不夠完善(已經有先執行訓練環境多收集到70多萬筆data)，train loss掉的很慢，而且還是蠻高的。找了很久仍然找不出問題，所以最後也沒有採用這個方法。\n",
    "\n",
    "### 4. Item-based Collaborative Filtering\n",
    "* 主要是會考量user與item之間的相似度，優先推薦相似度高的item給user。\n",
    "* 程式碼如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-Item Similarity Matrix\n",
    "\n",
    "在這個地方，我們是透過計算每一個user跟每一個item之間的餘弦相似度，並存成一個dataframe。舉例來說，row=10, item=100，這個對應到的值就是user 10跟item 100的相似度，越高代表這個user有越高的機率會喜歡這個item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './dataset/'\n",
    "if os.path.exists(dataset_dir + 'user_item_similarity.pkl'):\n",
    "    df_user_item_similarity = pd.read_pickle(dataset_dir + 'user_item_similarity.pkl')\n",
    "else:\n",
    "    with open(dataset_dir+'user_item_similarity.txt','a') as fin:\n",
    "        user_item_similarity_matrix = []\n",
    "        for user in range(N_USERS):\n",
    "            cosine_similarity_list = []\n",
    "            for item in range(N_ITEMS):\n",
    "                # calculating cosine similarity\n",
    "                cosine_similarity = (1-spatial.distance.cosine(df_user_embedding.iloc[user][\"concat_embeddings\"], df_item_embedding.iloc[item][\"concat_embeddings\"]))\n",
    "                cosine_similarity_list.append(cosine_similarity)    \n",
    "            \n",
    "            user_item_similarity_matrix.append(cosine_similarity_list)\n",
    "            fin.write(\",\".join([str(x) for x in cosine_similarity_list]))\n",
    "            fin.write('\\n')\n",
    "            fin.flush()\n",
    "        df_user_item_similarity = pd.DataFrame(user_item_similarity_matrix)\n",
    "        df_user_item_similarity.to_pickle(dataset_dir + 'user_item_similarity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>209517</th>\n",
       "      <th>209518</th>\n",
       "      <th>209519</th>\n",
       "      <th>209520</th>\n",
       "      <th>209521</th>\n",
       "      <th>209522</th>\n",
       "      <th>209523</th>\n",
       "      <th>209524</th>\n",
       "      <th>209525</th>\n",
       "      <th>209526</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.062287</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.261205</td>\n",
       "      <td>0.274883</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.013853</td>\n",
       "      <td>0.226747</td>\n",
       "      <td>0.061278</td>\n",
       "      <td>0.196566</td>\n",
       "      <td>0.109174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>0.059647</td>\n",
       "      <td>0.192302</td>\n",
       "      <td>0.062932</td>\n",
       "      <td>0.180494</td>\n",
       "      <td>0.012018</td>\n",
       "      <td>0.048684</td>\n",
       "      <td>0.079522</td>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.075266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.221764</td>\n",
       "      <td>0.055316</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>0.190925</td>\n",
       "      <td>0.091024</td>\n",
       "      <td>0.078525</td>\n",
       "      <td>0.017141</td>\n",
       "      <td>0.173482</td>\n",
       "      <td>0.051898</td>\n",
       "      <td>0.018566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004871</td>\n",
       "      <td>-0.035946</td>\n",
       "      <td>0.117543</td>\n",
       "      <td>-0.005578</td>\n",
       "      <td>0.029166</td>\n",
       "      <td>0.013565</td>\n",
       "      <td>0.055716</td>\n",
       "      <td>0.008542</td>\n",
       "      <td>0.073230</td>\n",
       "      <td>0.038990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106126</td>\n",
       "      <td>0.022207</td>\n",
       "      <td>0.117865</td>\n",
       "      <td>0.251699</td>\n",
       "      <td>-0.012615</td>\n",
       "      <td>-0.014647</td>\n",
       "      <td>0.126634</td>\n",
       "      <td>0.021103</td>\n",
       "      <td>0.089571</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036220</td>\n",
       "      <td>0.084124</td>\n",
       "      <td>-0.012249</td>\n",
       "      <td>0.043811</td>\n",
       "      <td>0.079477</td>\n",
       "      <td>-0.061269</td>\n",
       "      <td>0.037820</td>\n",
       "      <td>0.004233</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.029401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.093782</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>0.107896</td>\n",
       "      <td>0.075283</td>\n",
       "      <td>0.067360</td>\n",
       "      <td>-0.035875</td>\n",
       "      <td>0.099989</td>\n",
       "      <td>0.168689</td>\n",
       "      <td>0.128790</td>\n",
       "      <td>0.098932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056915</td>\n",
       "      <td>0.040844</td>\n",
       "      <td>0.170552</td>\n",
       "      <td>0.072513</td>\n",
       "      <td>0.178354</td>\n",
       "      <td>0.060635</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>0.203117</td>\n",
       "      <td>0.084404</td>\n",
       "      <td>0.080729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.122536</td>\n",
       "      <td>0.071247</td>\n",
       "      <td>0.153996</td>\n",
       "      <td>0.225814</td>\n",
       "      <td>0.183863</td>\n",
       "      <td>0.045011</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074886</td>\n",
       "      <td>0.209297</td>\n",
       "      <td>0.033176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063099</td>\n",
       "      <td>0.134406</td>\n",
       "      <td>0.060474</td>\n",
       "      <td>0.110714</td>\n",
       "      <td>0.092821</td>\n",
       "      <td>0.012666</td>\n",
       "      <td>0.129592</td>\n",
       "      <td>0.059556</td>\n",
       "      <td>0.067587</td>\n",
       "      <td>0.003772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 209527 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6       \\\n",
       "0  0.062287  0.003323  0.261205  0.274883  0.064868  0.013853  0.226747   \n",
       "1  0.221764  0.055316  0.112049  0.190925  0.091024  0.078525  0.017141   \n",
       "2  0.106126  0.022207  0.117865  0.251699 -0.012615 -0.014647  0.126634   \n",
       "3  0.093782  0.016448  0.107896  0.075283  0.067360 -0.035875  0.099989   \n",
       "4  0.122536  0.071247  0.153996  0.225814  0.183863  0.045011  0.210932   \n",
       "\n",
       "     7         8         9       ...    209517    209518    209519    209520  \\\n",
       "0  0.061278  0.196566  0.109174  ...  0.043999  0.059647  0.192302  0.062932   \n",
       "1  0.173482  0.051898  0.018566  ... -0.004871 -0.035946  0.117543 -0.005578   \n",
       "2  0.021103  0.089571  0.015671  ...  0.036220  0.084124 -0.012249  0.043811   \n",
       "3  0.168689  0.128790  0.098932  ...  0.056915  0.040844  0.170552  0.072513   \n",
       "4  0.074886  0.209297  0.033176  ...  0.063099  0.134406  0.060474  0.110714   \n",
       "\n",
       "     209521    209522    209523    209524    209525    209526  \n",
       "0  0.180494  0.012018  0.048684  0.079522 -0.015544  0.075266  \n",
       "1  0.029166  0.013565  0.055716  0.008542  0.073230  0.038990  \n",
       "2  0.079477 -0.061269  0.037820  0.004233  0.000226  0.029401  \n",
       "3  0.178354  0.060635  0.035779  0.203117  0.084404  0.080729  \n",
       "4  0.092821  0.012666  0.129592  0.059556  0.067587  0.003772  \n",
       "\n",
       "[5 rows x 209527 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 209527)\n"
     ]
    }
   ],
   "source": [
    "display(df_user_item_similarity.head(5))\n",
    "print(df_user_item_similarity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Item-based CF\n",
    "\n",
    "在訓練的時候我們主要用了兩個dataframe來做紀錄，第一個是`df_next`，另一個是`df_current`，在每一次的iteration開始前，`df_current`會從`df_next` copy一份，並在每一次要推薦使用者的時候，會根據 `df_current[user_id]`中相似度最高的前五個item優先進行推薦，推薦完以後這次的iteration就不該再推薦他，因此這邊就把下次的iteration設為1，而這次設為-1；那如果挑了五個推薦，結果這個user一個都沒點，就會給這些item一些penalty，所以把這些item的分數都乘上0.9。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_next = df_user_item_similarity\n",
    "model_save_dir = \"./model/item_based_CF\"\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 14896it [06:24, 38.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>0.0035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>0.0025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  avg_score\n",
       "0          0     0.0025\n",
       "1          1     0.0035\n",
       "2          2     0.0025\n",
       "3          3     0.0035\n",
       "4          4     0.0025\n",
       "..       ...        ...\n",
       "995      995     0.0035\n",
       "996      996     0.0025\n",
       "997      997     0.0025\n",
       "998      998     0.0025\n",
       "999      999     0.0025\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0055, 0.0065, 0.0025, 0.0035, 0.0025, 0.0025, 0.0045, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0035, 0.005, 0.0025, 0.0025, 0.0075, 0.0025, 0.0035, 0.0025, 0.004, 0.0045, 0.004, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.139, 0.0025, 0.005, 0.006, 0.0025, 0.005, 0.0025, 0.0025, 0.004, 0.005, 0.011, 0.0045, 0.0025, 0.0035, 0.014, 0.0025, 0.0025, 0.0075, 0.0045, 0.0025, 0.0025, 0.004, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0035, 0.0035, 0.0055, 0.0025, 0.005, 0.0025, 0.0045, 0.004, 0.0025, 0.0195, 0.0025, 0.0025, 0.004, 0.0035, 0.0035, 0.0025, 0.0025, 0.0325, 0.0025, 0.0085, 0.0035, 0.0035, 0.0035, 0.0035, 0.005, 0.0025, 0.004, 0.005, 0.0025, 0.0045, 0.0025, 0.0035, 0.0025, 0.0035, 0.004, 0.006, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.0025, 0.0045, 0.0025, 0.0025, 0.005, 0.0025, 0.0025, 0.004, 0.011, 0.0025, 0.004, 0.0035, 0.007, 0.0025, 0.019, 0.0025, 0.0085, 0.0025, 0.006, 0.0025, 0.0025, 0.0025, 0.0025, 0.0125, 0.0045, 0.0035, 0.0035, 0.0035, 0.004, 0.0025, 0.004, 0.0025, 0.008, 0.0085, 0.0035, 0.005, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.004, 0.0035, 0.0025, 0.0025, 0.008, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0065, 0.0055, 0.0035, 0.0025, 0.0025, 0.0035, 0.0035, 0.0035, 0.0035, 0.0055, 0.0025, 0.0025, 0.0035, 0.0035, 0.004, 0.0035, 0.0025, 0.0025, 0.0035, 0.005, 0.0115, 0.005, 0.1055, 0.0035, 0.0055, 0.0025, 0.0045, 0.0025, 0.0035, 0.0025, 0.0055, 0.024, 0.0025, 0.0025, 0.0025, 0.0045, 0.0025, 0.0025, 0.005, 0.0025, 0.007, 0.0035, 0.093, 0.0025, 0.0025, 0.011, 0.0025, 0.013, 0.0055, 0.0025, 0.0035, 0.004, 0.0045, 0.0025, 0.0025, 0.0025, 0.0045, 0.0025, 0.0025, 0.0045, 0.0025, 0.0025, 0.0045, 0.004, 0.0025, 0.0025, 0.0035, 0.0025, 0.023, 0.0035, 0.0025, 0.0025, 0.0025, 0.0085, 0.0025, 0.0235, 0.0155, 0.0025, 0.0025, 0.0025, 0.0025, 0.004, 0.005, 0.0035, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.008, 0.004, 0.004, 0.0025, 0.0335, 0.0035, 0.0035, 0.0025, 0.004, 0.004, 0.0065, 0.0025, 0.006, 0.0025, 0.0025, 0.0025, 0.004, 0.0035, 0.0035, 0.0025, 0.0185, 0.0035, 0.0025, 0.006, 0.0035, 0.006, 0.0535, 0.0045, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0055, 0.0025, 0.0035, 0.0025, 0.004, 0.0025, 0.004, 0.01, 0.0025, 0.0075, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.004, 0.004, 0.0025, 0.0055, 0.0035, 0.0035, 0.0025, 0.004, 0.0025, 0.149, 0.005, 0.004, 0.0175, 0.021, 0.0025, 0.0065, 0.093, 0.0025, 0.0025, 0.004, 0.0025, 0.0035, 0.0025, 0.0035, 0.0055, 0.0025, 0.0055, 0.004, 0.0035, 0.0025, 0.0025, 0.07, 0.0035, 0.0025, 0.0025, 0.0035, 0.005, 0.0035, 0.005, 0.005, 0.0035, 0.0025, 0.0025, 0.0025, 0.005, 0.0025, 0.0025, 0.0025, 0.0065, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0065, 0.0035, 0.0025, 0.007, 0.0025, 0.004, 0.0025, 0.0025, 0.0025, 0.0055, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.006, 0.0035, 0.0025, 0.0035, 0.0665, 0.004, 0.0045, 0.0025, 0.0085, 0.0025, 0.0025, 0.0025, 0.007, 0.011, 0.004, 0.0025, 0.005, 0.0025, 0.006, 0.004, 0.046, 0.004, 0.0025, 0.011, 0.0025, 0.0075, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.004, 0.0025, 0.0025, 0.0035, 0.004, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.007, 0.0025, 0.0035, 0.0035, 0.004, 0.0115, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0065, 0.0315, 0.0045, 0.0025, 0.0025, 0.0025, 0.008, 0.0045, 0.0035, 0.005, 0.0025, 0.0025, 0.0055, 0.0035, 0.0035, 0.0025, 0.006, 0.005, 0.0065, 0.0025, 0.0025, 0.0095, 0.0025, 0.005, 0.0025, 0.0025, 0.011, 0.0035, 0.0025, 0.005, 0.007, 0.0025, 0.0025, 0.0025, 0.004, 0.0025, 0.0045, 0.006, 0.004, 0.0025, 0.0025, 0.0025, 0.006, 0.0035, 0.0025, 0.0035, 0.0025, 0.205, 0.0025, 0.004, 0.0205, 0.0025, 0.0025, 0.0025, 0.004, 0.013, 0.0025, 0.0035, 0.0025, 0.004, 0.0025, 0.0025, 0.0215, 0.0045, 0.004, 0.0035, 0.006, 0.0025, 0.062, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.015, 0.0025, 0.0035, 0.0025, 0.0045, 0.0025, 0.004, 0.019, 0.0035, 0.0035, 0.0025, 0.004, 0.0035, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.007, 0.0025, 0.0025, 0.0025, 0.0045, 0.008, 0.0025, 0.0045, 0.0045, 0.271, 0.0035, 0.0035, 0.005, 0.0025, 0.02, 0.0025, 0.0035, 0.0035, 0.0025, 0.0025, 0.0045, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0065, 0.0025, 0.0045, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.005, 0.01, 0.0025, 0.0025, 0.015, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.004, 0.0025, 0.0045, 0.0025, 0.0035, 0.0055, 0.0085, 0.004, 0.0035, 0.004, 0.0025, 0.0025, 0.0025, 0.0025, 0.004, 0.004, 0.004, 0.0025, 0.004, 0.0025, 0.0025, 0.0025, 0.014, 0.0025, 0.0035, 0.0035, 0.0035, 0.0025, 0.0045, 0.0025, 0.005, 0.004, 0.0025, 0.0025, 0.0035, 0.0025, 0.006, 0.0025, 0.0025, 0.0205, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0035, 0.006, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.022, 0.0025, 0.0035, 0.004, 0.0025, 0.0035, 0.0035, 0.0035, 0.0025, 0.0195, 0.0025, 0.0025, 0.0025, 0.004, 0.0025, 0.0035, 0.0025, 0.0025, 0.005, 0.0025, 0.0025, 0.0035, 0.0395, 0.004, 0.006, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.005, 0.0025, 0.004, 0.0115, 0.005, 0.004, 0.0145, 0.0035, 0.007, 0.0235, 0.0025, 0.0035, 0.0025, 0.0045, 0.0035, 0.0025, 0.0115, 0.0025, 0.0035, 0.0025, 0.0045, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.006, 0.0025, 0.0035, 0.0035, 0.0025, 0.0025, 0.0025, 0.007, 0.0035, 0.0095, 0.0025, 0.0025, 0.0025, 0.004, 0.0065, 0.0025, 0.0025, 0.004, 0.0045, 0.0025, 0.004, 0.004, 0.005, 0.0035, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.004, 0.0365, 0.0065, 0.0175, 0.055, 0.007, 0.004, 0.008, 0.004, 0.0025, 0.0025, 0.0035, 0.0025, 0.016, 0.0025, 0.0025, 0.0025, 0.0025, 0.007, 0.0025, 0.0025, 0.0045, 0.0035, 0.0025, 0.006, 0.0035, 0.004, 0.004, 0.0025, 0.0035, 0.0035, 0.004, 0.004, 0.0045, 0.004, 0.0025, 0.0035, 0.0025, 0.0255, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0075, 0.0055, 0.004, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.004, 0.0025, 0.0025, 0.004, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0045, 0.0025, 0.0055, 0.0025, 0.0035, 0.004, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0065, 0.0325, 0.0045, 0.0025, 0.0045, 0.0025, 0.0025, 0.0025, 0.004, 0.0475, 0.004, 0.575, 0.0025, 0.0035, 0.0025, 0.023, 0.0025, 0.0025, 0.0035, 0.0055, 0.009, 0.0035, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0035, 0.005, 0.0025, 0.0025, 0.0055, 0.0165, 0.0025, 0.0025, 0.0035, 0.0035, 0.0035, 0.0025, 0.0025, 0.009, 0.004, 0.0035, 0.0025, 0.0045, 0.0025, 0.0025, 0.0035, 0.004, 0.005, 0.0055, 0.0025, 0.0025, 0.0025, 0.0035, 0.0035, 0.004, 0.0025, 0.0025, 0.006, 0.0055, 0.004, 0.0025, 0.005, 0.0025, 0.0035, 0.004, 0.006, 0.0025, 0.0025, 0.0035, 0.0025, 0.0045, 0.013, 0.0025, 0.0025, 0.0025, 0.0025, 0.004, 0.0035, 0.017, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025, 0.0095, 0.0025, 0.0065, 0.0025, 0.0025, 0.0035, 0.0045, 0.0195, 0.0025, 0.0025, 0.0035, 0.0045, 0.0025, 0.0025, 0.0025, 0.006, 0.008, 0.0025, 0.0035, 0.0025, 0.0025, 0.0035, 0.0035, 0.0035, 0.007, 0.0025, 0.0025, 0.01, 0.0025, 0.0435, 0.0075, 0.0025, 0.004, 1.0, 0.0025, 0.0025, 0.0145, 0.0045, 0.0035, 0.0025, 0.004, 0.013, 0.0035, 0.0035, 0.0045, 0.005, 0.004, 0.0035, 0.0045, 0.0045, 0.0025, 0.0025, 0.0035, 0.0025, 0.006, 0.0025, 0.106, 0.0035, 0.0035, 0.0035, 0.0035, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.0035, 0.0025, 0.0035, 0.0035, 0.0025, 0.005, 0.0025, 0.1005, 0.005, 0.0025, 0.0025, 0.004, 0.0025, 0.008, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025, 0.0035, 0.0025, 0.0025, 0.0025, 0.0025]]\n",
      "Total train score:7.448\n"
     ]
    }
   ],
   "source": [
    "# Initialize the training environment\n",
    "train_env = TrainingEnvironment()\n",
    "\n",
    "training_scores = []\n",
    "# Reset the training environment (this can be useful when you have finished one episode of simulation and do not want to re-initialize a new environment)\n",
    "train_env.reset()\n",
    "for epoch in range(1):\n",
    "    # APPLY CHANGE TO CURRENT DF.\n",
    "    df_current = df_next.copy()\n",
    "    with tqdm(desc='Training') as pbar:\n",
    "        # Check if there exist any active users in the environment\n",
    "        while (train_env.has_next_state()):\n",
    "            # Get the current user ID\n",
    "            user_id = train_env.get_state()\n",
    "            sorted_indices = np.argsort(df_current.iloc[user_id])\n",
    "            # Get top5 similarity response of recommending the slate to the current user\n",
    "            slate = list(sorted_indices[-5:])\n",
    "            clicked_id, in_environment = train_env.get_response(slate)\n",
    "            # Update similarity matrix here\n",
    "            if(clicked_id == -1): # mean there is no click in this item\n",
    "                for item in slate:\n",
    "                    df_current.iloc[user_id][item] *= 0.9\n",
    "                    df_next.iloc[user_id][item] *= 0.9\n",
    "            else:\n",
    "                for item in slate:\n",
    "                    if(item == clicked_id):\n",
    "                        df_current.iloc[user_id][item] = -1\n",
    "                        df_next.iloc[user_id][item] = 1\n",
    "                        \n",
    "            pbar.update(1)\n",
    "    training_scores.append(train_env.get_score())\n",
    "    df_next.to_pickle(f\"{model_save_dir}/epoch_{epoch}.pkl\")\n",
    "    train_env.reset()\n",
    "\n",
    "# Get the normalized session length score of all users\n",
    "avg_train_scores = [np.average(score) for score in zip(*training_scores)]\n",
    "df_train_score = pd.DataFrame([[user_id, score] for user_id, score in enumerate(avg_train_scores)], columns=['user_id', 'avg_score'])\n",
    "display(df_train_score)\n",
    "print(training_scores)\n",
    "total_score = df_train_score['avg_score'].sum()\n",
    "print(f\"Total train score:{total_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
